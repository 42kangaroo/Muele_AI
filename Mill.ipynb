{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Projektarbeit Mühle"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import PySimpleGUI as psGui\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import sklearn.preprocessing as pre\n",
    "from typing import List\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class definitions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-f5b4177051ad>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mMillEnv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobject\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misPlaying\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgamePhase\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mlist\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmoveNeeded\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-1-f5b4177051ad>\u001B[0m in \u001B[0;36mMillEnv\u001B[0;34m()\u001B[0m\n\u001B[1;32m    122\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0misFinished\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwinner\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m     \u001B[0;32mdef\u001B[0m \u001B[0mgetBoard\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    125\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mboard\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mgetInRows\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpos\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class MillEnv(object):\n",
    "    def __init__(self):\n",
    "        self.isPlaying: int = 1\n",
    "        self.gamePhase: list = [0,0]\n",
    "        self.moveNeeded: int = 0\n",
    "        self.inHand: list = [9,9]\n",
    "        self.onBoard: list = [0,0]\n",
    "        self.checkerPositions: list = [[],[]]\n",
    "        self.selected: int = -1\n",
    "        self.board: np.ndarray = np.zeros(24)\n",
    "        self.winner = 0\n",
    "        self.columns: np.ndarray = np.array([[0,1,2], # for determining how many checkers from each player are in a row\n",
    "                        [3,4,5],\n",
    "                        [6,7,8],\n",
    "                        [9,10,11],\n",
    "                        [12,13,14],\n",
    "                        [15,16,17],\n",
    "                        [18,19,20],\n",
    "                        [21,22,23],\n",
    "                        [0,9,21],\n",
    "                        [3,10,18],\n",
    "                        [6,11,15],\n",
    "                        [1,4,7],\n",
    "                        [16,19,22],\n",
    "                        [8,12,17],\n",
    "                        [5,13,20],\n",
    "                        [2,14,23],\n",
    "                        ])\n",
    "    def makeMove(self, move: int) -> bool:\n",
    "        valid: bool = False\n",
    "        last_state: tuple = self.getSummary(self.isPlaying)\n",
    "        last_player: int = self.isPlaying\n",
    "        if self.moveNeeded == 0: # Set Checker on position\n",
    "            if self.board[move] == 0:\n",
    "                self.board[move] = self.isPlaying\n",
    "                self.checkerPositions[1 if self.isPlaying == 1 else 0].append(move)\n",
    "                valid = True\n",
    "                if self.gamePhase[1 if self.isPlaying == 1 else 0] == 2:\n",
    "                    self.board[self.selected] = 0\n",
    "                    self.moveNeeded = 1\n",
    "                    self.checkerPositions[1 if self.isPlaying == 1 else 0].remove(self.selected)\n",
    "                else:\n",
    "                    self.inHand[1 if self.isPlaying == 1 else 0] -= 1\n",
    "                    self.onBoard[1 if self.isPlaying == 1 else 0] += 1\n",
    "                    if np.array(self.inHand).sum() == 0:\n",
    "                        self.gamePhase = [1,1]\n",
    "                        self.moveNeeded = 1\n",
    "                self.isPlaying = -self.isPlaying\n",
    "        elif self.moveNeeded == 1: # choose checker to move\n",
    "            if self.board[move] == 1 * self.isPlaying and (~(self.getMoveFields(move).all()) or self.gamePhase[1 if self.isPlaying == 1 else 0] == 2):\n",
    "                valid = True\n",
    "                self.selected = move\n",
    "                if self.gamePhase[1 if self.isPlaying == 1 else 0] == 2:\n",
    "                    self.moveNeeded = 0\n",
    "                else:\n",
    "                    self.moveNeeded = 2\n",
    "        elif self.moveNeeded == 2: # move checker up, down, left or right\n",
    "            if self.getMoveFields(self.selected)[move] == 0:\n",
    "                idxToMoveAxis: np.ndarray = np.where(self.getInRows(self.selected) == self.selected)\n",
    "                idxToMove = list(zip(idxToMoveAxis[1], idxToMoveAxis[0]))\n",
    "                order = idxToMove[0][1]\n",
    "                valid = True\n",
    "                self.board[self.selected] = 0\n",
    "                last_state = self.getSummary(last_player)\n",
    "                self.checkerPositions[1 if self.isPlaying == 1 else 0].remove(self.selected)\n",
    "                if move == 0: # up\n",
    "                    self.board[self.getInRows(self.selected)[1][idxToMove[abs(order-1)][0]-1]] = self.isPlaying\n",
    "                    self.checkerPositions[1 if self.isPlaying == 1 else 0].append(self.getInRows(self.selected)[1][idxToMove[abs(order-1)][0]-1])\n",
    "                if move == 1: # right\n",
    "                    self.board[self.getInRows(self.selected)[0][idxToMove[order][0]+1]] = self.isPlaying\n",
    "                    self.checkerPositions[1 if self.isPlaying == 1 else 0].append(self.getInRows(self.selected)[0][idxToMove[order][0]+1])\n",
    "                if move == 2: # down\n",
    "                    self.board[self.getInRows(self.selected)[1][idxToMove[abs(order-1)][0]+1]] = self.isPlaying\n",
    "                    self.checkerPositions[1 if self.isPlaying == 1 else 0].append(self.getInRows(self.selected)[1][idxToMove[abs(order-1)][0]+1])\n",
    "                if move == 3: # left\n",
    "                    self.board[self.getInRows(self.selected)[0][idxToMove[order][0]-1]] = self.isPlaying\n",
    "                    self.checkerPositions[1 if self.isPlaying == 1 else 0].append(self.getInRows(self.selected)[0][idxToMove[order][0]-1])\n",
    "                self.selected = -1\n",
    "                self.moveNeeded = 1\n",
    "                self.isPlaying = -self.isPlaying\n",
    "        elif self.moveNeeded == 3: # delete opponent checker\n",
    "            threeInChosenRow: np.ndarray = abs(self.board[self.getInRows(move)].sum(axis=1)) == 3\n",
    "            if self.board[move] == -1 * self.isPlaying and ~threeInChosenRow.any():\n",
    "                valid = True\n",
    "                self.board[move] = 0\n",
    "                self.checkerPositions[1 if self.isPlaying == -1 else 0].remove(move)\n",
    "                self.onBoard[1 if self.isPlaying == -1 else 0] -= 1\n",
    "                if self.gamePhase[1 if self.isPlaying == -1 else 0] == 0:\n",
    "                    self.moveNeeded = 0\n",
    "                elif self.gamePhase[1 if self.isPlaying == -1 else 0] == 1:\n",
    "                    if self.onBoard[1 if self.isPlaying == -1 else 0] == 3:\n",
    "                        self.gamePhase[1 if self.isPlaying == -1 else 0] = 2\n",
    "                    self.moveNeeded = 1\n",
    "                elif self.gamePhase[1 if self.isPlaying == -1 else 0] == 2:\n",
    "                    self.gamePhase = [3,3]\n",
    "                    self.winner = last_player\n",
    "                self.isPlaying = -self.isPlaying\n",
    "        if last_state[0] < self.getSummary(last_player)[0]:\n",
    "            self.isPlaying = last_player\n",
    "            self.moveNeeded = 3\n",
    "            canDelete: bool = False\n",
    "            for pos in self.checkerPositions[1 if self.isPlaying == -1 else 0]:\n",
    "                if ~(abs(self.board[self.getInRows(pos)].sum(axis=1)) == 3).any():\n",
    "                    canDelete = True\n",
    "                    break\n",
    "            if not canDelete:\n",
    "                valid = True\n",
    "                if self.gamePhase[1 if self.isPlaying == -1 else 0] == 0:\n",
    "                    self.moveNeeded = 0\n",
    "                elif self.gamePhase[1 if self.isPlaying == -1 else 0] >= 1:\n",
    "                    self.moveNeeded = 1\n",
    "        if self.gamePhase[1 if last_player == -1 else 0] == 1:\n",
    "            finished = True\n",
    "            for pos in self.checkerPositions[1 if last_player == -1 else 0]:\n",
    "                if ~(self.getMoveFields(pos).all()):\n",
    "                    finished = False\n",
    "                    break\n",
    "            if finished:\n",
    "                self.winner = last_player\n",
    "                self.gamePhase = [3,3]\n",
    "        return valid\n",
    "    def isFinished(self):\n",
    "        return self.winner\n",
    "    def getBoard(self) -> np.ndarray:\n",
    "        return copy.deepcopy(self.board)\n",
    "    def getInRows(self, pos: int) -> np.ndarray:\n",
    "        arrayPos: np.ndarray = self.columns == pos\n",
    "        return self.columns[arrayPos.any(axis=1)]\n",
    "    def reset(self):\n",
    "        self.isPlaying: int = 1\n",
    "        self.gamePhase: list = [0,0]\n",
    "        self.moveNeeded: int = 0\n",
    "        self.inHand: list = [9,9]\n",
    "        self.onBoard: list = [0,0]\n",
    "        self.checkerPositions: list = [[],[]]\n",
    "        self.selected: int = -1\n",
    "        self.board: np.ndarray = np.zeros(24)\n",
    "        self.winner = 0\n",
    "    def getSummary(self, player: int) -> (int, int, int, int):\n",
    "        numTwoPlayerActual: int = 0\n",
    "        numTwoPlayerOpponent: int = 0\n",
    "        numThreePlayerActual: int = 0\n",
    "        numThreePlayerOpponent: int = 0\n",
    "        for column in self.columns:\n",
    "            columnSum: int = self.board[column[0]] + self.board[column[1]] + self.board[column[2]]\n",
    "            if columnSum == -2 * player:\n",
    "                numTwoPlayerOpponent += 1\n",
    "            elif columnSum  == 2 * player:\n",
    "                numTwoPlayerActual += 1\n",
    "            elif columnSum == -3 * player:\n",
    "                numTwoPlayerOpponent += 1\n",
    "            elif columnSum == 3 * player:\n",
    "                numThreePlayerActual += 1\n",
    "        return numThreePlayerActual, numThreePlayerOpponent, numTwoPlayerActual, numTwoPlayerOpponent\n",
    "    def getMoveFields(self, pos: int) -> np.ndarray:\n",
    "        moveFields: np.ndarray = np.zeros(4)\n",
    "        chosenRows: np.ndarray = self.getInRows(pos)\n",
    "        idxAxis: np.ndarray = np.where(chosenRows == pos)\n",
    "        idx = list(zip(idxAxis[1], idxAxis[0]))\n",
    "        order = idx[0][1]\n",
    "        if idx[order][0] != 1:\n",
    "            if idx[order][0] == 0:\n",
    "                moveFields[3] = 2\n",
    "                moveFields[1] = self.board[chosenRows[0][1]]\n",
    "            elif idx[order][0] == 2:\n",
    "                moveFields[1] = 2\n",
    "                moveFields[3] = self.board[chosenRows[0][1]]\n",
    "        else:\n",
    "            moveFields[3] = self.board[chosenRows[0][0]]\n",
    "            moveFields[1] = self.board[chosenRows[0][2]]\n",
    "        if idx[abs(order-1)][0] != 1:\n",
    "            if idx[abs(order-1)][0] == 0:\n",
    "                moveFields[0] = 2\n",
    "                moveFields[2] = self.board[chosenRows[1][1]]\n",
    "            elif idx[abs(order-1)][0] == 2:\n",
    "                moveFields[2] = 2\n",
    "                moveFields[0] = self.board[chosenRows[1][1]]\n",
    "        else:\n",
    "            moveFields[0] = self.board[chosenRows[1][0]]\n",
    "            moveFields[2] = self.board[chosenRows[1][2]]\n",
    "        return moveFields\n",
    "    def getValidMoves(self) -> List[int]:\n",
    "        validList = []\n",
    "        if self.moveNeeded == 0:\n",
    "            validList = np.arange(24)\n",
    "            validList = np.setdiff1d(validList,self.checkerPositions[0])\n",
    "            validList = np.setdiff1d(validList,self.checkerPositions[1])\n",
    "        elif self.moveNeeded == 1:\n",
    "            validList = [pos for pos in self.checkerPositions[1 if self.isPlaying == 1 else 0] if ~self.getMoveFields(pos).all() or self.gamePhase[1 if self.isPlaying == 1 else 0] == 2]\n",
    "        elif self.moveNeeded == 2:\n",
    "            validList = [move for move in np.arange(4) if self.getMoveFields(self.selected)[move] == 0]\n",
    "        elif self.moveNeeded == 3:\n",
    "            validList = [pos for pos in self.checkerPositions[1 if self.isPlaying == -1 else 0] if ~(abs(self.board[self.getInRows(pos)].sum(axis=1)) == 3).any()]\n",
    "        return validList\n",
    "    def getReward(self, last_state, new_state, valid, to_train) -> float:\n",
    "        reward = 0.\n",
    "        if not valid:\n",
    "                return -.8\n",
    "        if to_train == 0 or to_train == 2:\n",
    "            if last_state[0] < new_state[0]:\n",
    "                return 1.\n",
    "            elif last_state[3] == new_state[3] and last_state[1] > 0:\n",
    "                return -.5\n",
    "            if last_state[2] < new_state[2]:\n",
    "                reward += .5\n",
    "            if last_state[3] > new_state[3]:\n",
    "                reward += .5\n",
    "        elif to_train == 1:\n",
    "            if valid:\n",
    "                return .1\n",
    "        elif to_train == 3:\n",
    "            if last_state[2] < new_state[2]:\n",
    "                return .7\n",
    "            elif last_state[3] > new_state[3]:\n",
    "                return .7\n",
    "        return reward\n",
    "    def getFullState(self):\n",
    "        return self.getBoard(), self.isPlaying, self.gamePhase, self.moveNeeded,self.inHand, self.onBoard, self.checkerPositions, self.selected, self.winner\n",
    "    def setFullState(self, board, isPlaying, gamePhase, moveNeded, inHand, onboard, checkerPositions, selected, winner):\n",
    "        self.isPlaying: int = isPlaying\n",
    "        self.gamePhase: list = gamePhase\n",
    "        self.moveNeeded: int = moveNeded\n",
    "        self.inHand: list = inHand\n",
    "        self.onBoard: list = onboard\n",
    "        self.checkerPositions: list = checkerPositions\n",
    "        self.selected: int = selected\n",
    "        self.board: np.ndarray = board\n",
    "        self.winner = winner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.encoder = pre.OneHotEncoder(sparse=False).fit(np.array([1,0,-1]).reshape(-1,1))\n",
    "    def processState(self, player, state, moveNeeded, selected= None):\n",
    "        state = state * player\n",
    "        state_OH = np.array(self.encoder.transform(state.reshape(-1,1)))\n",
    "        if moveNeeded == 2 and selected is not None:\n",
    "            state[selected] = 2\n",
    "            selectedArray = np.zeros(24)\n",
    "            selectedArray[selected] = 1\n",
    "            # TODO append selectedArray to state_OH\n",
    "            state_OH = np.append(state_OH, selectedArray.reshape(-1,1), axis=1)\n",
    "        return state_OH.flatten()\n",
    "        #return state\n",
    "    def getPos(self, state: np.ndarray, temp: float, validMoves: List[int],network:keras.Model=None) -> np.ndarray:\n",
    "        if network is None:\n",
    "            return np.random.choice(validMoves)\n",
    "        softmaxed_output = keras.backend.softmax(network(state.reshape(1,-1))/ temp)\n",
    "        action_value = np.random.choice(np.array(softmaxed_output[0]), p= np.array(softmaxed_output[0]))\n",
    "        pos: np.ndarray = np.argmax(softmaxed_output[0] == action_value)\n",
    "        return pos\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Graphics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class MillDisplayer(object):\n",
    "    def __init__(self, MillEnvironment: MillEnv = None):\n",
    "        psGui.theme(\"dark\")\n",
    "        self.millImage: str = \"MühleBrett.png\"\n",
    "        self.blackCheckerImage: str  = \"Schwarz.png\"\n",
    "        self.whiteCheckerImage: str = \"Weiss.png\"\n",
    "        self.millEnv: MillEnv = MillEnv()\n",
    "        if MillEnvironment is not None :\n",
    "            self.millEnv = MillEnvironment\n",
    "        self.ImageIDArray = np.array([])\n",
    "        self.imageLocations = [(10,490), (225, 490), (440, 490),\n",
    "                                (75,415), (225, 415), (375, 415),\n",
    "                                (150,340), (225, 340), (310, 340),\n",
    "                                (10,265), (75, 265), (150, 265),\n",
    "                                (310,265), (375, 265), (440, 265),\n",
    "                                (150,190), (225, 190), (310, 190),\n",
    "                                (75,115), (225, 115), (375, 115),\n",
    "                                (10,55), (225, 55), (440, 55)]\n",
    "        self.graph = psGui.Graph(\n",
    "                        canvas_size=(500, 500),\n",
    "                        graph_bottom_left=(0, 0),\n",
    "                        graph_top_right=(500, 500),\n",
    "                        )\n",
    "        self.statusTextBox = psGui.Text(\"Player \"+self.getPlayerName(self.millEnv.isPlaying)+\" is playing\", size=(50, 1))\n",
    "        self.layout_ = [[psGui.Button(\"Player vs. Player\"),psGui.Button(\"Player vs. Agent\"),psGui.Button(\"Agent vs. Agent\")],\n",
    "                        [self.statusTextBox],\n",
    "                       [self.graph],\n",
    "                       [psGui.Button(\"Close\")]]\n",
    "        self.window  = psGui.Window(\"Mill AI\", layout=self.layout_)\n",
    "        self.window.finalize()\n",
    "        self.graph.DrawImage(filename=self.millImage, location=(0,500))\n",
    "        self.activateClick()\n",
    "        self.reloadEnv()\n",
    "    def windowsLoop(self):\n",
    "        while True:\n",
    "            event, values = self.window.read()\n",
    "            if event == psGui.WIN_CLOSED or event == 'Close': # if user closes window or clicks cancel\n",
    "                break\n",
    "            elif not event == \"\":\n",
    "                self.reset()\n",
    "        self.window.close()\n",
    "    def makeMove(self, pos: int) -> bool:\n",
    "        valid: bool = self.millEnv.makeMove(pos)\n",
    "        if valid:\n",
    "            self.reloadEnv()\n",
    "        return valid\n",
    "    def reloadEnv(self):\n",
    "        self.setStatus(\"Player \" + self.getPlayerName(self.millEnv.isPlaying) + \" is playing - move needed: \" + str(self.millEnv.moveNeeded))\n",
    "        for imageID in self.ImageIDArray:\n",
    "            self.graph.DeleteFigure(imageID)\n",
    "        np.delete(self.ImageIDArray, np.s_[:])\n",
    "        for case, location in zip(self.millEnv.getBoard(), self.imageLocations):\n",
    "            if case == 1:\n",
    "                self.ImageIDArray = np.append(self.ImageIDArray,self.graph.DrawImage(filename=self.blackCheckerImage, location=location))\n",
    "            elif case == -1:\n",
    "                self.ImageIDArray = np.append(self.ImageIDArray,self.graph.DrawImage(filename=self.whiteCheckerImage, location=location))\n",
    "        self.window.refresh()\n",
    "    def getClicked(self, event) -> int:\n",
    "        for index, location in enumerate(self.imageLocations):\n",
    "            x2, y2 = location\n",
    "            if self.isInArea(event.x, -event.y + 500, x2, y2, 50, 50):\n",
    "                return index\n",
    "        return -1\n",
    "    def setAfterClicked(self, event):\n",
    "        pos = self.getClicked(event)\n",
    "        if pos == -1:\n",
    "            return False\n",
    "        if self.millEnv.moveNeeded == 2:\n",
    "            dif = self.millEnv.selected - pos\n",
    "            if dif == 0:\n",
    "                return False\n",
    "            if dif == -1:\n",
    "                pos = 1\n",
    "            elif dif == 1:\n",
    "                pos = 3\n",
    "            elif dif < 0:\n",
    "                pos = 2\n",
    "            elif dif > 0:\n",
    "                pos = 0\n",
    "        return self.makeMove(pos)\n",
    "    def isInArea(self, posX1: int, posY1: int, posX2: int, posY2: int, width: int, height: int) -> bool:\n",
    "        if posX2 <= posX1 <= posX2 + width:\n",
    "            if posY2 >= posY1 >= posY2 - height:\n",
    "                return True\n",
    "        return False\n",
    "    def setStatus(self, status: str):\n",
    "        self.statusTextBox.Update(status)\n",
    "    def close(self):\n",
    "        self.window.close()\n",
    "    def activateClick(self):\n",
    "        self.graph.TKCanvas.bind(\"<Button-1>\",self.setAfterClicked)\n",
    "    def deactivateClick(self):\n",
    "        self.graph.TKCanvas.unbind(\"<Button-1>\")\n",
    "    def read(self, timout: bool=False):\n",
    "        return self.window.read(1 if timout else None)\n",
    "    def reset(self):\n",
    "        self.millEnv.reset()\n",
    "        self.reloadEnv()\n",
    "    def getPlayerName(self, player: int) -> str:\n",
    "        if player == 1:\n",
    "            return \"black\"\n",
    "        elif player == -1:\n",
    "            return \"white\"\n",
    "        else:\n",
    "            return \"not a player\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Moderated Graphics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class ModeratedGraphics(object):\n",
    "    def __init__(self, modelList: List[keras.Model] = [None,None,None,None]):\n",
    "        self.env = MillEnv()\n",
    "        self.agent = Agent()\n",
    "        self.graphics = MillDisplayer(self.env)\n",
    "        self.graphics.reloadEnv()\n",
    "        self.modelList = modelList\n",
    "    def agentPlay(self):\n",
    "        self.env.reset()\n",
    "        self.graphics.deactivateClick()\n",
    "        finished = 0\n",
    "        while finished == 0:\n",
    "            pos = self.agent.getPos(self.env.getBoard(),1, self.env.getValidMoves(),self.modelList[self.env.moveNeeded])\n",
    "            self.graphics.makeMove(pos)\n",
    "            event, values = self.graphics.read(True)\n",
    "            if self.eventHandler(event):\n",
    "                return\n",
    "            finished = self.env.isFinished()\n",
    "        if not finished == 2:\n",
    "            self.graphics.setStatus(\"player \" + self.graphics.getPlayerName(finished) +\" won\")\n",
    "        else:\n",
    "            self.graphics.setStatus(\"The game ended in a draw\")\n",
    "    def playersVSPlayer(self):\n",
    "        self.graphics.activateClick()\n",
    "        self.graphics.reset()\n",
    "        finished = 0\n",
    "        while finished == 0:\n",
    "            event, values = self.graphics.read(True)\n",
    "            if self.eventHandler(event):\n",
    "                return\n",
    "            self.graphics.reloadEnv()\n",
    "            finished = self.env.isFinished()\n",
    "        if not finished == 2:\n",
    "            self.graphics.setStatus(\"player \" + self.graphics.getPlayerName(finished) +\" won\")\n",
    "        else:\n",
    "            self.graphics.setStatus(\"The game ended in a draw\")\n",
    "        self.graphics.deactivateClick()\n",
    "    def playerVSAgent(self):\n",
    "        self.graphics.activateClick()\n",
    "        self.graphics.reset()\n",
    "        finished = 0\n",
    "        while finished == 0:\n",
    "            event, values = self.graphics.read(True)\n",
    "            if self.eventHandler(event):\n",
    "                return\n",
    "            elif self.env.isPlaying == 1:\n",
    "                self.graphics.activateClick()\n",
    "            else:\n",
    "                self.graphics.deactivateClick()\n",
    "                pos = self.agent.getPos(self.env.getBoard(),1, self.env.getValidMoves(),self.modelList[self.env.moveNeeded])\n",
    "                self.graphics.makeMove(pos)\n",
    "            self.graphics.reloadEnv()\n",
    "            finished = self.env.isFinished()\n",
    "        if not finished == 2:\n",
    "            self.graphics.setStatus(\"player \" + self.graphics.getPlayerName(finished) +\" won\")\n",
    "        else:\n",
    "            self.graphics.setStatus(\"The game ended in a draw\")\n",
    "        self.graphics.deactivateClick()\n",
    "    def playLoop(self):\n",
    "        self.graphics.deactivateClick()\n",
    "        self.playerVSAgent()\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            event, values = self.graphics.read()\n",
    "            finished = self.eventHandler(event)\n",
    "    def eventHandler(self, event) -> bool:\n",
    "        if event == psGui.WIN_CLOSED or event == 'Close': # if user closes window or clicks cancel\n",
    "            self.graphics.close()\n",
    "            return True\n",
    "        elif event == \"Agent vs. Agent\":\n",
    "            self.agentPlay()\n",
    "        elif event == \"Player vs. Player\":\n",
    "            self.playersVSPlayer()\n",
    "        elif event == \"Player vs. Agent\":\n",
    "            self.playerVSAgent()\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, size: int):\n",
    "        self.size = size\n",
    "        self.curr_write_idx = 0\n",
    "        self.available_samples = 0\n",
    "        self.buffer = np.array([(np.zeros(24,dtype=np.float32), 0.0, 0.0, np.zeros(24,\n",
    "                                dtype=np.float32), False) for index in range(self.size)], dtype=object)\n",
    "        self.base_node, self.leaf_nodes = create_tree([0 for index in range(self.size)])\n",
    "        self.frame_idx = 0\n",
    "        self.action_idx = 1\n",
    "        self.reward_idx = 2\n",
    "        self.terminal_idx = 3\n",
    "        self.beta = 0.4\n",
    "        self.alpha = 0.6\n",
    "        self.min_priority = 0.01\n",
    "\n",
    "    def append(self, experience: tuple, priority: float):\n",
    "        self.buffer[self.curr_write_idx] = experience\n",
    "        self.update(self.curr_write_idx, priority)\n",
    "        self.curr_write_idx += 1\n",
    "        # reset the current writer position index if creater than the allowed size\n",
    "        if self.curr_write_idx >= self.size:\n",
    "            self.curr_write_idx = 0\n",
    "        # max out available samples at the memory buffer size\n",
    "        if self.available_samples + 1 < self.size:\n",
    "            self.available_samples += 1\n",
    "        else:\n",
    "            self.available_samples = self.size - 1\n",
    "\n",
    "    def update(self, idx: int, priority: float):\n",
    "        update(self.leaf_nodes[idx], self.adjust_priority(priority))\n",
    "\n",
    "    def adjust_priority(self, priority: float):\n",
    "        return np.power(priority + self.min_priority, self.alpha)\n",
    "\n",
    "    def sample(self, num_samples: int):\n",
    "        sampled_idxs = []\n",
    "        is_weights = []\n",
    "        sample_no = 0\n",
    "        while sample_no < num_samples:\n",
    "            sample_val = np.random.uniform(0, self.base_node.value)\n",
    "            samp_node = retrieve(sample_val, self.base_node)\n",
    "            if samp_node.idx < self.available_samples - 1:\n",
    "                sampled_idxs.append(samp_node.idx)\n",
    "                p = samp_node.value / self.base_node.value\n",
    "                is_weights.append((self.available_samples + 1) * p)\n",
    "                sample_no += 1\n",
    "        # apply the beta factor and normalise so that the maximum is_weight < 1\n",
    "        is_weights = np.array(is_weights)\n",
    "        is_weights = np.power(is_weights, -self.beta)\n",
    "        is_weights = is_weights / np.max(is_weights)\n",
    "        # now load up the state and next state variables according to sampled idxs\n",
    "        return self.buffer[sampled_idxs], sampled_idxs, is_weights\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class DQModel(keras.Model):\n",
    "    def __init__(self, hidden_size: int, num_actions: int, dueling: bool, dropoutRate: float):\n",
    "        super(DQModel, self).__init__()\n",
    "        self.dueling = dueling\n",
    "        self.dropoutRate = dropoutRate\n",
    "        self.dense1 = keras.layers.Dense(hidden_size *3, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.dense2 = keras.layers.Dense(hidden_size * 4, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.dense3 = keras.layers.Dense(hidden_size * 5, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.dense4 = keras.layers.Dense(hidden_size * 4, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.adv_dense1 = keras.layers.Dense(hidden_size * 3, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.adv_dense2 = keras.layers.Dense(hidden_size * 4, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.adv_dense3 = keras.layers.Dense(hidden_size * 3, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.adv_out = keras.layers.Dense(num_actions,\n",
    "                                          kernel_initializer=keras.initializers.he_normal())\n",
    "        if dueling:\n",
    "            self.v_dense1 = keras.layers.Dense(hidden_size * 3, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "            self.v_dense2 = keras.layers.Dense(hidden_size * 4, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "            self.v_out = keras.layers.Dense(1, kernel_initializer=keras.initializers.he_normal())\n",
    "            self.lambda_layer = keras.layers.Lambda(lambda x: x- tf.reduce_mean(x))\n",
    "            self.combine = keras.layers.Add()\n",
    "\n",
    "    def call(self, input, **kwargs):\n",
    "        x = self.dense1(input)\n",
    "        x = keras.backend.dropout(x, self.dropoutRate)\n",
    "        x = self.dense2(x)\n",
    "        x = keras.backend.dropout(x, self.dropoutRate)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        adv = self.adv_dense1(x)\n",
    "        adv = self.adv_dense2(adv)\n",
    "        adv = self.adv_dense3(adv)\n",
    "        adv = self.adv_out(adv)\n",
    "        if self.dueling:\n",
    "            v = self.v_dense1(x)\n",
    "            v = self.v_dense2(v)\n",
    "            v = self.v_out(v)\n",
    "            norm_adv = self.lambda_layer(adv)\n",
    "            combined = self.combine([v, norm_adv])\n",
    "            return combined\n",
    "        return adv\n",
    "\n",
    "    @tf.function\n",
    "    def traceable(self, input, **kwargs):\n",
    "        return self(input, **kwargs)\n",
    "\n",
    "    def changeDropoutRate(self, rate):\n",
    "        self.dropoutRate = rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Node"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right, is_leaf: bool = False, idx = None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.is_leaf = is_leaf\n",
    "        self.value = sum(n.value for n in (left, right) if n is not None)\n",
    "        self.parent = None\n",
    "        self.idx = idx  # this value is only set for leaf nodes\n",
    "        if left is not None:\n",
    "            left.parent = self\n",
    "        if right is not None:\n",
    "            right.parent = self\n",
    "\n",
    "    @classmethod\n",
    "    def create_leaf(cls, value, idx):\n",
    "        leaf = cls(None, None, is_leaf=True, idx=idx)\n",
    "        leaf.value = value\n",
    "        return leaf\n",
    "\n",
    "\n",
    "def create_tree(input: list):\n",
    "    nodes = [Node.create_leaf(v, i) for i, v in enumerate(input)]\n",
    "    leaf_nodes = nodes\n",
    "    while len(nodes) > 1:\n",
    "        inodes = iter(nodes)\n",
    "        nodes = [Node(*pair) for pair in zip(inodes, inodes)]\n",
    "\n",
    "    return nodes[0], leaf_nodes\n",
    "\n",
    "def retrieve(value: float, node: Node):\n",
    "    if node.is_leaf:\n",
    "        return node\n",
    "\n",
    "    if node.left.value >= value:\n",
    "        return retrieve(value, node.left)\n",
    "    else:\n",
    "        return retrieve(value - node.left.value, node.right)\n",
    "\n",
    "def update(node: Node, new_value: float):\n",
    "    change = new_value - node.value\n",
    "\n",
    "    node.value = new_value\n",
    "    propagate_changes(change, node.parent)\n",
    "\n",
    "\n",
    "def propagate_changes(change: float, node: Node):\n",
    "    node.value += change\n",
    "\n",
    "    if node.parent is not None:\n",
    "        propagate_changes(change, node.parent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Controler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def huber_loss(loss):\n",
    "    return 0.5 * loss ** 2 if abs(loss) < 1.0 else abs(loss) - 0.5\n",
    "\n",
    "class Controler(object):\n",
    "    def __init__(self, min_dropout, max_dropout, dropout_min_iter, min_temp, max_temp, temp_min_iter, min_beta, max_beta, beta_min_iter,  gamma, batch_size, tau, num_actions, memory_samples, store_path, hidden_layers, delay_training, to_train):\n",
    "        self.to_train = to_train\n",
    "        self.delay_training = delay_training\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.store_path = store_path\n",
    "        self.memory_samples = memory_samples\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.beta_min_iter = beta_min_iter\n",
    "        self.max_beta = max_beta\n",
    "        self.min_beta = min_beta\n",
    "        self.temp_min_iter = temp_min_iter\n",
    "        self.max_temp = max_temp\n",
    "        self.min_temp = min_temp\n",
    "        self.dropout_min_iter = dropout_min_iter\n",
    "        self.max_dropout = max_dropout\n",
    "        self.min_dropout = min_dropout\n",
    "        self.num_actions = num_actions\n",
    "        self.primary_network = DQModel(self.hidden_layers, self.num_actions, True, self.max_dropout)\n",
    "        self.target_network = DQModel(self.hidden_layers, self.num_actions, True, 0.)\n",
    "        self.primary_network.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.Huber())\n",
    "        for t, e in zip(self.target_network.trainable_variables, self.primary_network.trainable_variables):\n",
    "            t.assign(e)\n",
    "        self.env = MillEnv()\n",
    "        self.agent = Agent()\n",
    "        self.memory = Memory(memory_samples)\n",
    "        self.steps = 0\n",
    "        self.temp = max_temp\n",
    "        self.dropoutRate = max_dropout\n",
    "        self.train_writer = tf.summary.create_file_writer(\"Tensorboard/\" + self.store_path)\n",
    "    def startGame(self):\n",
    "        self.env.reset()\n",
    "    def update_network(self):\n",
    "        # update target network parameters slowly from primary network\n",
    "        for t, e in zip(self.target_network.trainable_variables, self.primary_network.trainable_variables):\n",
    "            t.assign(t * (1 - self.tau) + e * self.tau)\n",
    "    def get_per_error(self, batch, primary_network, target_network):\n",
    "        states = np.array([val[0] for val in batch])\n",
    "        actions = np.array([val[1] for val in batch])\n",
    "        rewards = np.array([val[2] for val in batch])\n",
    "        next_states = np.array([val[3] for val in batch])\n",
    "        valid_idxs = np.array([val[4] for val in batch])\n",
    "        # predict Q(s,a) given the batch of states\n",
    "        prim_qt = primary_network(states)\n",
    "        # predict Q(s',a') from the evaluation network\n",
    "        prim_qtp1 = primary_network(next_states)\n",
    "        # copy the prim_qt tensor into the target_q tensor - we then will update one index corresponding to the max action\n",
    "        target_q = prim_qt.numpy()\n",
    "        updates = rewards\n",
    "        batch_idxs = np.arange(len(batch))\n",
    "        prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "        q_from_target = target_network(next_states)\n",
    "        updates[valid_idxs] += self.gamma * q_from_target.numpy()[batch_idxs[valid_idxs], prim_action_tp1[valid_idxs]]\n",
    "        target_q[batch_idxs, actions] = updates\n",
    "        error = [huber_loss(target_q[i, actions[i]] - prim_qt.numpy()[i, actions[i]])for i in range(states.shape[0])]\n",
    "        return target_q, error\n",
    "    def train_network(self, primary_network, memory, target_network):\n",
    "        batch, idxs, is_weights = memory.sample(self.batch_size)\n",
    "        states = np.array([val[0] for val in batch])\n",
    "        target_q, error = self.get_per_error(batch, primary_network, target_network )\n",
    "        for i in range(len(idxs)):\n",
    "            memory.update(idxs[i], error[i])\n",
    "        loss = primary_network.train_on_batch(states, target_q, is_weights)\n",
    "        return loss\n",
    "    def gameLoop(self, episode):\n",
    "        self.startGame()\n",
    "        totreward = 0.\n",
    "        last_state = [0,0]\n",
    "        last_action = [0,0]\n",
    "        reward = [0,0]\n",
    "        cnt = 1\n",
    "        avg_loss = 0\n",
    "        invalid_moves = 0\n",
    "        while True:\n",
    "            if self.env.moveNeeded == self.to_train:\n",
    "                self.primary_network.changeDropoutRate(self.dropoutRate)\n",
    "                actual_player = self.env.isPlaying\n",
    "                last_state[1 if actual_player == 1 else 0] = self.env.getBoard(), self.env.getSummary(actual_player)\n",
    "                pos= self.agent.getPos(self.agent.processState(actual_player, self.env.getBoard(), self.to_train, self.env.selected), self.temp, self.env.getValidMoves(),self.primary_network)\n",
    "                valid = self.env.makeMove(pos)\n",
    "                reward[1 if actual_player == 1 else 0] = self.env.getReward(last_state[1 if actual_player == 1 else 0][1], self.env.getSummary(actual_player), valid, self.to_train)\n",
    "                last_action[1 if actual_player == 1 else 0] = pos, valid\n",
    "                finished = self.env.isFinished()\n",
    "                if last_state[1 if actual_player == -1 else 0] != 0 and valid:\n",
    "                    new_state = self.env.getBoard(), self.env.getSummary(-actual_player)\n",
    "                    rewardAbs = self.env.getReward(last_state[1 if actual_player == -1 else 0][1], new_state[1], last_action[1 if actual_player == -1 else 0][1], self.to_train) if finished != 0 else reward[1 if actual_player == -1 else 0]\n",
    "                    totreward += rewardAbs\n",
    "                    sample = (self.agent.processState(-actual_player,last_state[1 if actual_player == -1 else 0][0], self.to_train, self.env.selected), last_action[1 if actual_player == -1 else 0][0], rewardAbs, self.agent.processState(-actual_player,new_state[0], self.to_train, self.env.selected), finished == 0)\n",
    "                    _, error = self.get_per_error([sample], self.primary_network, self.target_network)\n",
    "                    self.memory.append(sample,error[0])\n",
    "                if self.steps > self.delay_training:\n",
    "                    loss = self.train_network(self.primary_network, self.memory, self.target_network)\n",
    "                    self.update_network()\n",
    "                else:\n",
    "                    loss = 0\n",
    "                avg_loss += loss\n",
    "                if self.steps > self.delay_training:\n",
    "                    self.temp = self.max_temp - ((self.steps - self.delay_training) / self.temp_min_iter) * \\\n",
    "                      (self.max_temp - self.min_temp) if self.steps < self.temp_min_iter else \\\n",
    "                    self.min_temp\n",
    "                    self.dropoutRate = self.max_dropout - ((self.steps - self.delay_training) / self.dropout_min_iter) * \\\n",
    "                      (self.max_dropout - self.min_dropout) if self.steps < self.dropout_min_iter else \\\n",
    "                    self.min_dropout\n",
    "                    beta = self.min_beta + ((self.steps - self.delay_training) / self.beta_min_iter) * \\\n",
    "                      (self.max_beta - self.min_beta) if self.steps < self.beta_min_iter else \\\n",
    "                    self.max_beta\n",
    "                    self.memory.beta = beta\n",
    "                self.steps += 1\n",
    "                if cnt >= 1000:\n",
    "                    finished = 2\n",
    "                if  not valid or finished != 0:\n",
    "                    invalid_moves += 0 if finished != 0 else 1\n",
    "                    new_state = self.env.getBoard()\n",
    "                    totreward += reward[1 if actual_player == 1 else 0]\n",
    "                    sample = (self.agent.processState(actual_player, last_state[1 if actual_player == 1 else 0][0], self.to_train, self.env.selected), last_action[1 if actual_player == 1 else 0][0], reward[1 if actual_player == 1 else 0],self.agent.processState(actual_player,new_state, self.to_train, self.env.selected), False)\n",
    "                    _, error = self.get_per_error([sample], self.primary_network, self.target_network)\n",
    "                    self.memory.append(sample, error[0])\n",
    "                cnt += 1\n",
    "            else:\n",
    "                self.env.makeMove(self.agent.getPos(self.env.getBoard(), 1,self.env.getValidMoves()))\n",
    "            if self.env.isFinished() != 0:\n",
    "                finished = self.env.isFinished()\n",
    "                if self.steps > self.delay_training:\n",
    "                    avg_loss /= cnt\n",
    "                    print(f\"Episode: {episode}, Reward: {totreward:.1f}, invalid moves: {invalid_moves}, avg loss: {avg_loss:.3f}, eps: {self.temp:.3f}, dropout: {self.dropoutRate:.3f}, beta: {self.memory.beta:.3f}\")\n",
    "                else:\n",
    "                    avg_loss = 0\n",
    "                    print(f\"Pre-Training... Episode {episode}\")\n",
    "                if episode == 0:\n",
    "                    tf.summary.trace_on()\n",
    "                    self.primary_network.traceable(self.agent.processState(1, last_state[1][0],self.to_train, 0).reshape(1,-1))\n",
    "                with self.train_writer.as_default():\n",
    "                    tf.summary.scalar('reward', totreward, step=episode)\n",
    "                    tf.summary.scalar('avg loss', avg_loss, step=episode)\n",
    "                    tf.summary.scalar('invalid moves', invalid_moves, step=episode)\n",
    "                    if episode == 0:\n",
    "                        tf.summary.trace_export('model', step=0)\n",
    "                break\n",
    "        return finished, reward\n",
    "    def train(self, episodes: int):\n",
    "        for index in range(episodes):\n",
    "            self.gameLoop(index)\n",
    "        self.primary_network.save_weights(f\"models/\" + self.store_path, save_format=\"tf\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Monte Carlo Tree Search"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MonteCarloTreeSearch(object):\n",
    "    def __init__(self, node):\n",
    "        self.root = node\n",
    "    def best_action(self, simulations_number):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        simulations_number : int\n",
    "            number of simulations performed to get the best action\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        for _ in range(simulations_number):\n",
    "            v = self._tree_policy()\n",
    "            reward = v.rollout()\n",
    "            v.backpropagate(reward)\n",
    "        # to select best child go for exploitation only\n",
    "        return self.root.best_child(c_param=0.)\n",
    "\n",
    "    def _tree_policy(self):\n",
    "        \"\"\"\n",
    "        selects node to run rollout/playout for\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        current_node = self.root\n",
    "        while not current_node.is_terminal_node():\n",
    "            if not current_node.is_fully_expanded():\n",
    "                return current_node.expand()\n",
    "            else:\n",
    "                current_node = current_node.best_child()\n",
    "        return current_node\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Node"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<tokenize>\"\u001B[0;36m, line \u001B[0;32m35\u001B[0m\n\u001B[0;31m    def is_terminal_node(self):\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class State(object):\n",
    "    def __init__(self, state, valid_moves, terminal, env, last_move, parent = None):\n",
    "        self.last_move = last_move\n",
    "        self.n = 0\n",
    "        self.q = 0\n",
    "        self.terminal = terminal\n",
    "        self.valid_moves = valid_moves\n",
    "        self.untried_actions: list =self.valid_moves\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.results = defaultdict(float)\n",
    "        self.env: MillEnv = env\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.untried_actions) == 0\n",
    "    def best_child(self, c_param=1.4):\n",
    "        choices_weights = [\n",
    "            (c.q / c.n) + c_param * np.sqrt((2 * np.log(self.n) / c.n))\n",
    "            for c in self.children\n",
    "        ]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "    def rollout_policy(self):\n",
    "        return self.valid_moves[np.random.randint(len(self.valid_moves))]\n",
    "    def is_terminal_node(self):\n",
    "        return self.terminal\n",
    "\n",
    "    def expand(self):\n",
    "        action = self.untried_actions.pop()\n",
    "        self.env.setFullState(self.state[0], self.state[1],self.state[2], self.state[3],self.state[4], self.state[5],self.state[6], self.state[7], self.state[8])\n",
    "        self.env.makeMove(action)\n",
    "        next_state = self.env.getFullState()\n",
    "        child_node = State(next_state, self.env.getValidMoves(), self.env.isFinished(), self.env,action, parent=self)\n",
    "        self.children.append(child_node)\n",
    "        return child_node\n",
    "    def rollout(self, discount):\n",
    "        self.env.setFullState(self.state[0], self.state[1],self.state[2], self.state[3],self.state[4], self.state[5],self.state[6], self.state[7], self.state[8])\n",
    "        reward = 0\n",
    "        iter = 0\n",
    "        while env.isFinished() ==0:\n",
    "            last_player = self.env.isPlaying\n",
    "            last_state = self.env.getSummary(last_player)\n",
    "            last_move = self.env.moveNeeded\n",
    "            action = self.rollout_policy()\n",
    "            self.env.makeMove(action)\n",
    "            reward += math.pow(discount, iter) * self.env.getReward(last_state, self.env.getSummary(last_player), True,last_move)\n",
    "            iter += 1\n",
    "        if env.isFinished() == self.state[1]:\n",
    "            reward += 5\n",
    "        elif env.isFinished() == -self.state[1]:\n",
    "            reward -= 5\n",
    "        return reward\n",
    "\n",
    "    def backpropagate(self, result):\n",
    "        self.n += 1.\n",
    "        self.results[result] += 1.\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 13 14]\n",
      " [ 2 14 23]]\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
      "[ 1.  1.  1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n",
      "0\n",
      "False\n",
      "[ 1.  0.  0. -1.]\n",
      "(1, 0, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "env = MillEnv()\n",
    "print(env.getInRows(14))\n",
    "\n",
    "env.makeMove(0)\n",
    "env.makeMove(3)\n",
    "env.makeMove(2)\n",
    "env.makeMove(4)\n",
    "env.makeMove(1)\n",
    "env.makeMove(3)\n",
    "env.makeMove(3)\n",
    "print(env.getValidMoves())\n",
    "print(env.getBoard())\n",
    "print(env.moveNeeded)\n",
    "ThreeInChosenRow = abs(env.board[env.getInRows(0)].sum(axis=1)) == 3\n",
    "print(~ThreeInChosenRow.any())\n",
    "print(env.getMoveFields(4))\n",
    "print(env.getSummary(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MillEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-226506c6c4e0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0menv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMillEnv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mag\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAgent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mag\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocessState\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetBoard\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mfirstTime\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0miters\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m100\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'MillEnv' is not defined"
     ]
    }
   ],
   "source": [
    "env = MillEnv()\n",
    "ag = Agent()\n",
    "print(ag.processState(1,env.getBoard(), 2,3).shape)\n",
    "firstTime = time.time()\n",
    "iters = 100\n",
    "for i in range(iters):\n",
    "    env.reset()\n",
    "    while env.isFinished() == 0:\n",
    "        validNow = env.makeMove(ag.getPos(env.getBoard(), 0, env.getValidMoves()))\n",
    "        if not validNow:\n",
    "            print(\"invalid\")\n",
    "    print(f\"Gewonnen hat {env.isFinished()}\")\n",
    "print(time.time()- firstTime)\n",
    "print((time.time()-firstTime)/iters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Displayer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MillDisplayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-605db04b24a9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdisplayer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMillDisplayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdisplayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwindowsLoop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'MillDisplayer' is not defined"
     ]
    }
   ],
   "source": [
    "displayer = MillDisplayer()\n",
    "displayer.windowsLoop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Moderated  Play"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModeratedGraphics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-0504e9ed1958>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmoderateGraphics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mModeratedGraphics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mmoderateGraphics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplayLoop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ModeratedGraphics' is not defined"
     ]
    }
   ],
   "source": [
    "moderateGraphics = ModeratedGraphics()\n",
    "moderateGraphics.playLoop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Controller"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dq_model_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dq_model_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Pre-Training... Episode 0\n",
      "Pre-Training... Episode 1\n",
      "Episode: 2, Reward: -16.0, invalid moves: 4, avg loss: 0.361, eps: 4.000, dropout: 0.400, beta: 0.400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-61-7f21387700b4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mcont\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mControler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0.4\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m70000\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0.7\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m90000\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0.4\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m60000\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.9\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m64\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0.1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m24\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m100000\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34mf\"Move0-{dt.datetime.now().strftime('%d%m%Y%H%M')}\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m256\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mcont\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-34-172f2153abce>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, episodes)\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepisodes\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mindex\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepisodes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 147\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgameLoop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    148\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprimary_network\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave_weights\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"models/\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstore_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msave_format\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"tf\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mgetReward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-34-172f2153abce>\u001B[0m in \u001B[0;36mgameLoop\u001B[0;34m(self, episode)\u001B[0m\n\u001B[1;32m    119\u001B[0m                     \u001B[0mtotreward\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mactual_player\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    120\u001B[0m                     \u001B[0msample\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocessState\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mactual_player\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_state\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mactual_player\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselected\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_action\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mactual_player\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mactual_player\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocessState\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mactual_player\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnew_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselected\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 121\u001B[0;31m                     \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merror\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_per_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprimary_network\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_network\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    122\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmemory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merror\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m                 \u001B[0mcnt\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-34-172f2153abce>\u001B[0m in \u001B[0;36mget_per_error\u001B[0;34m(self, batch, primary_network, target_network)\u001B[0m\n\u001B[1;32m     49\u001B[0m         \u001B[0mprim_qt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprimary_network\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstates\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;31m# predict Q(s',a') from the evaluation network\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mprim_qtp1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprimary_network\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnext_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;31m# copy the prim_qt tensor into the target_q tensor - we then will update one index corresponding to the max action\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m         \u001B[0mtarget_q\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprim_qt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/own_networks/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    966\u001B[0m           with base_layer_utils.autocast_context_manager(\n\u001B[1;32m    967\u001B[0m               self._compute_dtype):\n\u001B[0;32m--> 968\u001B[0;31m             \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcast_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    969\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    970\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_set_mask_metadata\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-32-2751722d6d89>\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, input, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdense3\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdense4\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m         \u001B[0madv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madv_dense1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m         \u001B[0madv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madv_dense2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0madv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m         \u001B[0madv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madv_dense3\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0madv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/own_networks/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    966\u001B[0m           with base_layer_utils.autocast_context_manager(\n\u001B[1;32m    967\u001B[0m               self._compute_dtype):\n\u001B[0;32m--> 968\u001B[0;31m             \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcast_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    969\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    970\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_set_mask_metadata\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_masks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/own_networks/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msparse_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparse_tensor_dense_matmul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkernel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1193\u001B[0m       \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1194\u001B[0;31m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmat_mul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkernel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muse_bias\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m       \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias_add\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/own_networks/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36mmat_mul\u001B[0;34m(a, b, transpose_a, transpose_b, name)\u001B[0m\n\u001B[1;32m   5562\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5563\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5564\u001B[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[1;32m   5565\u001B[0m         \u001B[0m_ctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_context_handle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtld\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"MatMul\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5566\u001B[0m         \u001B[0mtld\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mop_callbacks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"transpose_a\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtranspose_a\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"transpose_b\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "cont = Controler(0.05,0.4,70000,0.7,4,90000,0.4,1,60000, 0.9, 64,0.1,24,100000,f\"Move0-{dt.datetime.now().strftime('%d%m%Y%H%M')}\", 256,100,0)\n",
    "cont.train(2000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}