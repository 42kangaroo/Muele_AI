{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Projektarbeit MÃ¼hle"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import PySimpleGUI as psGui\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import random\n",
    "import sklearn.preprocessing as pre\n",
    "from typing import List"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class definitions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "class MillEnv(object):\n",
    "    def __init__(self):\n",
    "        self.isPlaying: int = 1\n",
    "        self.gamePhase: list = [0,0]\n",
    "        self.moveNeeded: int = 0\n",
    "        self.inHand: list = [9,9]\n",
    "        self.onBoard: list = [0,0]\n",
    "        self.checkerPositions: list = [[],[]]\n",
    "        self.selected: int = -1\n",
    "        self.board: np.ndarray = np.zeros(24)\n",
    "        self.winner = 0\n",
    "        self.columns: np.ndarray = np.array([[0,1,2], # for determining how many checkers from each player are in a row\n",
    "                        [3,4,5],\n",
    "                        [6,7,8],\n",
    "                        [9,10,11],\n",
    "                        [12,13,14],\n",
    "                        [15,16,17],\n",
    "                        [18,19,20],\n",
    "                        [21,22,23],\n",
    "                        [0,9,21],\n",
    "                        [3,10,18],\n",
    "                        [6,11,15],\n",
    "                        [1,4,7],\n",
    "                        [16,19,22],\n",
    "                        [8,12,17],\n",
    "                        [5,13,20],\n",
    "                        [2,14,23],\n",
    "                        ])\n",
    "    def makeMove(self, move: int) -> bool:\n",
    "        valid: bool = False\n",
    "        last_state: tuple = self.getSummary(self.isPlaying)\n",
    "        last_player: int = self.isPlaying\n",
    "        if self.moveNeeded == 0: # Set Checker on position\n",
    "            if self.board[move] == 0:\n",
    "                self.board[move] = self.isPlaying\n",
    "                self.checkerPositions[1 if self.isPlaying == 1 else 0].append(move)\n",
    "                valid = True\n",
    "                if self.gamePhase[1 if self.isPlaying == 1 else 0] == 2:\n",
    "                    self.board[self.selected] = 0\n",
    "                    self.moveNeeded = 1\n",
    "                    self.checkerPositions[1 if self.isPlaying == 1 else 0].remove(self.selected)\n",
    "                else:\n",
    "                    self.inHand[1 if self.isPlaying == 1 else 0] -= 1\n",
    "                    self.onBoard[1 if self.isPlaying == 1 else 0] += 1\n",
    "                    if np.array(self.inHand).sum() == 0:\n",
    "                        self.gamePhase = [1,1]\n",
    "                        self.moveNeeded = 1\n",
    "                self.isPlaying = -self.isPlaying\n",
    "        elif self.moveNeeded == 1: # choose checker to move\n",
    "            if self.board[move] == 1 * self.isPlaying and (~(self.getMoveFields(move).all()) or self.gamePhase[1 if self.isPlaying == 1 else 0] == 2):\n",
    "                valid = True\n",
    "                self.selected = move\n",
    "                if self.gamePhase[1 if self.isPlaying == 1 else 0] == 2:\n",
    "                    self.moveNeeded = 0\n",
    "                else:\n",
    "                    self.moveNeeded = 2\n",
    "        elif self.moveNeeded == 2: # move checker up, down, left or right\n",
    "            if self.getMoveFields(self.selected)[move] == 0:\n",
    "                idxToMoveAxis: np.ndarray = np.where(self.getInRows(self.selected) == self.selected)\n",
    "                idxToMove = list(zip(idxToMoveAxis[1], idxToMoveAxis[0]))\n",
    "                order = idxToMove[0][1]\n",
    "                valid = True\n",
    "                self.board[self.selected] = 0\n",
    "                last_state = self.getSummary(last_player)\n",
    "                self.checkerPositions[1 if self.isPlaying == 1 else 0].remove(self.selected)\n",
    "                if move == 0: # up\n",
    "                    self.board[self.getInRows(self.selected)[1][idxToMove[abs(order-1)][0]-1]] = self.isPlaying\n",
    "                    self.checkerPositions[1 if self.isPlaying == 1 else 0].append(self.getInRows(self.selected)[1][idxToMove[abs(order-1)][0]-1])\n",
    "                if move == 1: # right\n",
    "                    self.board[self.getInRows(self.selected)[0][idxToMove[order][0]+1]] = self.isPlaying\n",
    "                    self.checkerPositions[1 if self.isPlaying == 1 else 0].append(self.getInRows(self.selected)[0][idxToMove[order][0]+1])\n",
    "                if move == 2: # down\n",
    "                    self.board[self.getInRows(self.selected)[1][idxToMove[abs(order-1)][0]+1]] = self.isPlaying\n",
    "                    self.checkerPositions[1 if self.isPlaying == 1 else 0].append(self.getInRows(self.selected)[1][idxToMove[abs(order-1)][0]+1])\n",
    "                if move == 3: # left\n",
    "                    self.board[self.getInRows(self.selected)[0][idxToMove[order][0]-1]] = self.isPlaying\n",
    "                    self.checkerPositions[1 if self.isPlaying == 1 else 0].append(self.getInRows(self.selected)[0][idxToMove[order][0]-1])\n",
    "                self.selected = -1\n",
    "                self.moveNeeded = 1\n",
    "                self.isPlaying = -self.isPlaying\n",
    "        elif self.moveNeeded == 3: # delete opponent checker\n",
    "            threeInChosenRow: np.ndarray = abs(self.board[self.getInRows(move)].sum(axis=1)) == 3\n",
    "            if self.board[move] == -1 * self.isPlaying and ~threeInChosenRow.any():\n",
    "                valid = True\n",
    "                self.board[move] = 0\n",
    "                self.checkerPositions[1 if self.isPlaying == -1 else 0].remove(move)\n",
    "                self.onBoard[1 if self.isPlaying == -1 else 0] -= 1\n",
    "                if self.gamePhase[1 if self.isPlaying == -1 else 0] == 0:\n",
    "                    self.moveNeeded = 0\n",
    "                elif self.gamePhase[1 if self.isPlaying == -1 else 0] == 1:\n",
    "                    if self.onBoard[1 if self.isPlaying == -1 else 0] == 3:\n",
    "                        self.gamePhase[1 if self.isPlaying == -1 else 0] = 2\n",
    "                    self.moveNeeded = 1\n",
    "                elif self.gamePhase[1 if self.isPlaying == -1 else 0] == 2:\n",
    "                    self.gamePhase = [3,3]\n",
    "                    self.winner = last_player\n",
    "                self.isPlaying = -self.isPlaying\n",
    "        if last_state[0] < self.getSummary(last_player)[0]:\n",
    "            self.isPlaying = last_player\n",
    "            self.moveNeeded = 3\n",
    "            canDelete: bool = False\n",
    "            for pos in self.checkerPositions[1 if self.isPlaying == -1 else 0]:\n",
    "                if ~(abs(self.board[self.getInRows(pos)].sum(axis=1)) == 3).any():\n",
    "                    canDelete = True\n",
    "                    break\n",
    "            if not canDelete:\n",
    "                valid = True\n",
    "                if self.gamePhase[1 if self.isPlaying == -1 else 0] == 0:\n",
    "                    self.moveNeeded = 0\n",
    "                elif self.gamePhase[1 if self.isPlaying == -1 else 0] >= 1:\n",
    "                    self.moveNeeded = 1\n",
    "        if self.gamePhase[1 if last_player == -1 else 0] == 1:\n",
    "            finished = True\n",
    "            for pos in self.checkerPositions[1 if last_player == -1 else 0]:\n",
    "                if ~(self.getMoveFields(pos).all()):\n",
    "                    finished = False\n",
    "                    break\n",
    "            if finished:\n",
    "                self.winner = last_player\n",
    "                self.gamePhase = [3,3]\n",
    "        return valid\n",
    "    def isFinished(self):\n",
    "        return self.winner\n",
    "    def getBoard(self) -> np.ndarray:\n",
    "        return copy.deepcopy(self.board)\n",
    "    def getInRows(self, pos: int) -> np.ndarray:\n",
    "        arrayPos: np.ndarray = self.columns == pos\n",
    "        return self.columns[arrayPos.any(axis=1)]\n",
    "    def reset(self):\n",
    "        self.isPlaying: int = 1\n",
    "        self.gamePhase: list = [0,0]\n",
    "        self.moveNeeded: int = 0\n",
    "        self.inHand: list = [9,9]\n",
    "        self.onBoard: list = [0,0]\n",
    "        self.checkerPositions: list = [[],[]]\n",
    "        self.selected: int = -1\n",
    "        self.board: np.ndarray = np.zeros(24)\n",
    "        self.winner = 0\n",
    "    def getSummary(self, player: int) -> (int, int, int, int):\n",
    "        numTwoPlayerActual: int = 0\n",
    "        numTwoPlayerOpponent: int = 0\n",
    "        numThreePlayerActual: int = 0\n",
    "        numThreePlayerOpponent: int = 0\n",
    "        for column in self.columns:\n",
    "            columnSum: int = self.board[column[0]] + self.board[column[1]] + self.board[column[2]]\n",
    "            if columnSum == -2 * player:\n",
    "                numTwoPlayerOpponent += 1\n",
    "            elif columnSum  == 2 * player:\n",
    "                numTwoPlayerActual += 1\n",
    "            elif columnSum == -3 * player:\n",
    "                numTwoPlayerOpponent += 1\n",
    "            elif columnSum == 3 * player:\n",
    "                numThreePlayerActual += 1\n",
    "        return numThreePlayerActual, numThreePlayerOpponent, numTwoPlayerActual, numTwoPlayerOpponent\n",
    "    def getMoveFields(self, pos: int) -> np.ndarray:\n",
    "        moveFields: np.ndarray = np.zeros(4)\n",
    "        chosenRows: np.ndarray = self.getInRows(pos)\n",
    "        idxAxis: np.ndarray = np.where(chosenRows == pos)\n",
    "        idx = list(zip(idxAxis[1], idxAxis[0]))\n",
    "        order = idx[0][1]\n",
    "        if idx[order][0] != 1:\n",
    "            if idx[order][0] == 0:\n",
    "                moveFields[3] = 2\n",
    "                moveFields[1] = self.board[chosenRows[0][1]]\n",
    "            elif idx[order][0] == 2:\n",
    "                moveFields[1] = 2\n",
    "                moveFields[3] = self.board[chosenRows[0][1]]\n",
    "        else:\n",
    "            moveFields[3] = self.board[chosenRows[0][0]]\n",
    "            moveFields[1] = self.board[chosenRows[0][2]]\n",
    "        if idx[abs(order-1)][0] != 1:\n",
    "            if idx[abs(order-1)][0] == 0:\n",
    "                moveFields[0] = 2\n",
    "                moveFields[2] = self.board[chosenRows[1][1]]\n",
    "            elif idx[abs(order-1)][0] == 2:\n",
    "                moveFields[2] = 2\n",
    "                moveFields[0] = self.board[chosenRows[1][1]]\n",
    "        else:\n",
    "            moveFields[0] = self.board[chosenRows[1][0]]\n",
    "            moveFields[2] = self.board[chosenRows[1][2]]\n",
    "        return moveFields\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.encoder = pre.OneHotEncoder(sparse=False).fit(np.array([1,0,-1]).reshape(-1,1))\n",
    "    def processState(self, player, state, moveNeeded, selected= None):\n",
    "        state = state * player\n",
    "        state_OH = np.array(self.encoder.transform(state.reshape(-1,1)))\n",
    "        if moveNeeded == 2:\n",
    "            selectedArray = np.zeros(24)\n",
    "            selectedArray[selected] = 1\n",
    "            # TODO append selectedArray to state_OH\n",
    "            state_OH = np.append(state_OH, selectedArray.reshape(-1,1), axis=1)\n",
    "        return state_OH.flatten()\n",
    "    def getPos(self, state: np.ndarray, temp: float, moveNeeded: int,network:keras.Model=None) -> np.ndarray:\n",
    "        if network is None:\n",
    "            if moveNeeded == 2:\n",
    "                return np.random.randint(0,4)\n",
    "            else:\n",
    "                return np.random.randint(0,24)\n",
    "        softmaxed_output = keras.backend.softmax(network(state.reshape(1,-1))/ temp)\n",
    "        action_value = np.random.choice(np.array(softmaxed_output[0]), p= np.array(softmaxed_output[0]))\n",
    "        pos: np.ndarray = np.argmax(softmaxed_output[0] == action_value)\n",
    "        return pos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Graphics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MillDisplayer(object):\n",
    "    def __init__(self, MillEnvironment: MillEnv = None):\n",
    "        psGui.theme(\"dark\")\n",
    "        self.millImage: str = \"MÃ¼hleBrett.png\"\n",
    "        self.blackCheckerImage: str  = \"Schwarz.png\"\n",
    "        self.whiteCheckerImage: str = \"Weiss.png\"\n",
    "        self.millEnv: MillEnv = MillEnv()\n",
    "        if MillEnvironment is not None :\n",
    "            self.millEnv = MillEnvironment\n",
    "        self.ImageIDArray = np.array([])\n",
    "        self.imageLocations = [(10,490), (225, 490), (440, 490),\n",
    "                                (75,415), (225, 415), (375, 415),\n",
    "                                (150,340), (225, 340), (310, 340),\n",
    "                                (10,265), (75, 265), (150, 265),\n",
    "                                (310,265), (375, 265), (440, 265),\n",
    "                                (150,190), (225, 190), (310, 190),\n",
    "                                (75,115), (225, 115), (375, 115),\n",
    "                                (10,55), (225, 55), (440, 55)]\n",
    "        self.graph = psGui.Graph(\n",
    "                        canvas_size=(500, 500),\n",
    "                        graph_bottom_left=(0, 0),\n",
    "                        graph_top_right=(500, 500),\n",
    "                        )\n",
    "        self.statusTextBox = psGui.Text(\"Player \"+self.getPlayerName(self.millEnv.isPlaying)+\" is playing\", size=(50, 1))\n",
    "        self.layout_ = [[psGui.Button(\"Player vs. Player\"),psGui.Button(\"Player vs. Agent\"),psGui.Button(\"Agent vs. Agent\")],\n",
    "                        [self.statusTextBox],\n",
    "                       [self.graph],\n",
    "                       [psGui.Button(\"Close\")]]\n",
    "        self.window  = psGui.Window(\"Mill AI\", layout=self.layout_)\n",
    "        self.window.finalize()\n",
    "        self.graph.DrawImage(filename=self.millImage, location=(0,500))\n",
    "        self.activateClick()\n",
    "        self.reloadEnv()\n",
    "    def windowsLoop(self):\n",
    "        while True:\n",
    "            event, values = self.window.read()\n",
    "            if event == psGui.WIN_CLOSED or event == 'Close': # if user closes window or clicks cancel\n",
    "                break\n",
    "            elif not event == \"\":\n",
    "                self.reset()\n",
    "        self.window.close()\n",
    "    def makeMove(self, pos: int) -> bool:\n",
    "        valid: bool = self.millEnv.makeMove(pos)\n",
    "        if valid:\n",
    "            self.reloadEnv()\n",
    "        return valid\n",
    "    def reloadEnv(self):\n",
    "        self.setStatus(\"Player \" + self.getPlayerName(self.millEnv.isPlaying) + \" is playing - move needed: \" + str(self.millEnv.moveNeeded))\n",
    "        for imageID in self.ImageIDArray:\n",
    "            self.graph.DeleteFigure(imageID)\n",
    "        np.delete(self.ImageIDArray, np.s_[:])\n",
    "        for case, location in zip(self.millEnv.getBoard(), self.imageLocations):\n",
    "            if case == 1:\n",
    "                self.ImageIDArray = np.append(self.ImageIDArray,self.graph.DrawImage(filename=self.blackCheckerImage, location=location))\n",
    "            elif case == -1:\n",
    "                self.ImageIDArray = np.append(self.ImageIDArray,self.graph.DrawImage(filename=self.whiteCheckerImage, location=location))\n",
    "        self.window.refresh()\n",
    "    def getClicked(self, event) -> int:\n",
    "        for index, location in enumerate(self.imageLocations):\n",
    "            x2, y2 = location\n",
    "            if self.isInArea(event.x, -event.y + 500, x2, y2, 50, 50):\n",
    "                return index\n",
    "        return -1\n",
    "    def setAfterClicked(self, event):\n",
    "        pos = self.getClicked(event)\n",
    "        if pos == -1:\n",
    "            return False\n",
    "        if self.millEnv.moveNeeded == 2:\n",
    "            dif = self.millEnv.selected - pos\n",
    "            if dif == 0:\n",
    "                return False\n",
    "            if dif == -1:\n",
    "                pos = 1\n",
    "            elif dif == 1:\n",
    "                pos = 3\n",
    "            elif dif < 0:\n",
    "                pos = 2\n",
    "            elif dif > 0:\n",
    "                pos = 0\n",
    "        return self.makeMove(pos)\n",
    "    def isInArea(self, posX1: int, posY1: int, posX2: int, posY2: int, width: int, height: int) -> bool:\n",
    "        if posX2 <= posX1 <= posX2 + width:\n",
    "            if posY2 >= posY1 >= posY2 - height:\n",
    "                return True\n",
    "        return False\n",
    "    def setStatus(self, status: str):\n",
    "        self.statusTextBox.Update(status)\n",
    "    def close(self):\n",
    "        self.window.close()\n",
    "    def activateClick(self):\n",
    "        self.graph.TKCanvas.bind(\"<Button-1>\",self.setAfterClicked)\n",
    "    def deactivateClick(self):\n",
    "        self.graph.TKCanvas.unbind(\"<Button-1>\")\n",
    "    def read(self, timout: bool=False):\n",
    "        return self.window.read(1 if timout else None)\n",
    "    def reset(self):\n",
    "        self.millEnv.reset()\n",
    "        self.reloadEnv()\n",
    "    def getPlayerName(self, player: int) -> str:\n",
    "        if player == 1:\n",
    "            return \"black\"\n",
    "        elif player == -1:\n",
    "            return \"white\"\n",
    "        else:\n",
    "            return \"not a player\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Moderated Graphics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ModeratedGraphics(object):\n",
    "    def __init__(self, modelList: List[keras.Model] = [None,None,None,None]):\n",
    "        self.env = MillEnv()\n",
    "        self.agent = Agent()\n",
    "        self.graphics = MillDisplayer(self.env)\n",
    "        self.graphics.reloadEnv()\n",
    "        self.modelList = modelList\n",
    "    def agentPlay(self):\n",
    "        self.env.reset()\n",
    "        self.graphics.deactivateClick()\n",
    "        finished = 0\n",
    "        while finished == 0:\n",
    "            pos = self.agent.getPos(self.env.getBoard(),1, self.env.moveNeeded,self.modelList[self.env.moveNeeded])\n",
    "            self.graphics.makeMove(pos)\n",
    "            event, values = self.graphics.read(True)\n",
    "            if self.eventHandler(event):\n",
    "                return\n",
    "            finished = self.env.isFinished()\n",
    "        if not finished == 2:\n",
    "            self.graphics.setStatus(\"player \" + self.graphics.getPlayerName(finished) +\" won\")\n",
    "        else:\n",
    "            self.graphics.setStatus(\"The game ended in a draw\")\n",
    "    def playersVSPlayer(self):\n",
    "        self.graphics.activateClick()\n",
    "        self.graphics.reset()\n",
    "        finished = 0\n",
    "        while finished == 0:\n",
    "            event, values = self.graphics.read(True)\n",
    "            if self.eventHandler(event):\n",
    "                return\n",
    "            self.graphics.reloadEnv()\n",
    "            finished = self.env.isFinished()\n",
    "        if not finished == 2:\n",
    "            self.graphics.setStatus(\"player \" + self.graphics.getPlayerName(finished) +\" won\")\n",
    "        else:\n",
    "            self.graphics.setStatus(\"The game ended in a draw\")\n",
    "        self.graphics.deactivateClick()\n",
    "    def playerVSAgent(self):\n",
    "        self.graphics.activateClick()\n",
    "        self.graphics.reset()\n",
    "        finished = 0\n",
    "        while finished == 0:\n",
    "            event, values = self.graphics.read(True)\n",
    "            if self.eventHandler(event):\n",
    "                return\n",
    "            elif self.env.isPlaying == 1:\n",
    "                self.graphics.activateClick()\n",
    "            else:\n",
    "                self.graphics.deactivateClick()\n",
    "                pos = self.agent.getPos(self.env.getBoard(),1, self.env.moveNeeded,self.modelList[self.env.moveNeeded])\n",
    "                self.graphics.makeMove(pos)\n",
    "            self.graphics.reloadEnv()\n",
    "            finished = self.env.isFinished()\n",
    "        if not finished == 2:\n",
    "            self.graphics.setStatus(\"player \" + self.graphics.getPlayerName(finished) +\" won\")\n",
    "        else:\n",
    "            self.graphics.setStatus(\"The game ended in a draw\")\n",
    "        self.graphics.deactivateClick()\n",
    "    def playLoop(self):\n",
    "        self.graphics.deactivateClick()\n",
    "        self.playerVSAgent()\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            event, values = self.graphics.read()\n",
    "            finished = self.eventHandler(event)\n",
    "    def eventHandler(self, event) -> bool:\n",
    "        if event == psGui.WIN_CLOSED or event == 'Close': # if user closes window or clicks cancel\n",
    "            self.graphics.close()\n",
    "            return True\n",
    "        elif event == \"Agent vs. Agent\":\n",
    "            self.agentPlay()\n",
    "        elif event == \"Player vs. Player\":\n",
    "            self.playersVSPlayer()\n",
    "        elif event == \"Player vs. Agent\":\n",
    "            self.playerVSAgent()\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, size: int):\n",
    "        self.size = size\n",
    "        self.curr_write_idx = 0\n",
    "        self.available_samples = 0\n",
    "        self.buffer = np.array([(np.zeros(24,dtype=np.float32), 0.0, 0.0, np.zeros(24,\n",
    "                                dtype=np.float32), False) for index in range(self.size)], dtype=object)\n",
    "        self.base_node, self.leaf_nodes = create_tree([0 for index in range(self.size)])\n",
    "        self.frame_idx = 0\n",
    "        self.action_idx = 1\n",
    "        self.reward_idx = 2\n",
    "        self.terminal_idx = 3\n",
    "        self.beta = 0.4\n",
    "        self.alpha = 0.6\n",
    "        self.min_priority = 0.01\n",
    "\n",
    "    def append(self, experience: tuple, priority: float):\n",
    "        self.buffer[self.curr_write_idx] = experience\n",
    "        self.update(self.curr_write_idx, priority)\n",
    "        self.curr_write_idx += 1\n",
    "        # reset the current writer position index if creater than the allowed size\n",
    "        if self.curr_write_idx >= self.size:\n",
    "            self.curr_write_idx = 0\n",
    "        # max out available samples at the memory buffer size\n",
    "        if self.available_samples + 1 < self.size:\n",
    "            self.available_samples += 1\n",
    "        else:\n",
    "            self.available_samples = self.size - 1\n",
    "\n",
    "    def update(self, idx: int, priority: float):\n",
    "        update(self.leaf_nodes[idx], self.adjust_priority(priority))\n",
    "\n",
    "    def adjust_priority(self, priority: float):\n",
    "        return np.power(priority + self.min_priority, self.alpha)\n",
    "\n",
    "    def sample(self, num_samples: int):\n",
    "        sampled_idxs = []\n",
    "        is_weights = []\n",
    "        sample_no = 0\n",
    "        while sample_no < num_samples:\n",
    "            sample_val = np.random.uniform(0, self.base_node.value)\n",
    "            samp_node = retrieve(sample_val, self.base_node)\n",
    "            if samp_node.idx < self.available_samples - 1:\n",
    "                sampled_idxs.append(samp_node.idx)\n",
    "                p = samp_node.value / self.base_node.value\n",
    "                is_weights.append((self.available_samples + 1) * p)\n",
    "                sample_no += 1\n",
    "        # apply the beta factor and normalise so that the maximum is_weight < 1\n",
    "        is_weights = np.array(is_weights)\n",
    "        is_weights = np.power(is_weights, -self.beta)\n",
    "        is_weights = is_weights / np.max(is_weights)\n",
    "        # now load up the state and next state variables according to sampled idxs\n",
    "        return self.buffer[sampled_idxs], sampled_idxs, is_weights\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class DQModel(keras.Model):\n",
    "    def __init__(self, hidden_size: int, num_actions: int, dueling: bool, dropoutRate: float):\n",
    "        super(DQModel, self).__init__()\n",
    "        self.dueling = dueling\n",
    "        self.dropoutRate = dropoutRate\n",
    "        self.dense1 = keras.layers.Dense(hidden_size *3, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.dense2 = keras.layers.Dense(hidden_size * 4, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.dense3 = keras.layers.Dense(hidden_size * 5, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.dense4 = keras.layers.Dense(hidden_size * 4, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.adv_dense1 = keras.layers.Dense(hidden_size * 3, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.adv_dense2 = keras.layers.Dense(hidden_size * 4, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.adv_dense3 = keras.layers.Dense(hidden_size * 3, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.adv_out = keras.layers.Dense(num_actions,\n",
    "                                          kernel_initializer=keras.initializers.he_normal())\n",
    "        if dueling:\n",
    "            self.v_dense1 = keras.layers.Dense(hidden_size * 3, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "            self.v_dense2 = keras.layers.Dense(hidden_size * 4, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "            self.v_out = keras.layers.Dense(1, kernel_initializer=keras.initializers.he_normal())\n",
    "            self.lambda_layer = keras.layers.Lambda(lambda x: x- tf.reduce_mean(x))\n",
    "            self.combine = keras.layers.Add()\n",
    "\n",
    "    def call(self, input, **kwargs):\n",
    "        x = self.dense1(input)\n",
    "        x = keras.backend.dropout(x, self.dropoutRate)\n",
    "        x = self.dense2(x)\n",
    "        x = keras.backend.dropout(x, self.dropoutRate)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        adv = self.adv_dense1(x)\n",
    "        adv = self.adv_dense2(adv)\n",
    "        adv = self.adv_dense3(adv)\n",
    "        adv = self.adv_out(adv)\n",
    "        if self.dueling:\n",
    "            v = self.v_dense1(x)\n",
    "            v = self.v_dense2(v)\n",
    "            v = self.v_out(v)\n",
    "            norm_adv = self.lambda_layer(adv)\n",
    "            combined = self.combine([v, norm_adv])\n",
    "            return combined\n",
    "        return adv\n",
    "\n",
    "    @tf.function\n",
    "    def traceable(self, input, **kwargs):\n",
    "        return self(input, **kwargs)\n",
    "\n",
    "    def changeDropoutRate(self, rate):\n",
    "        self.dropoutRate = rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Node"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right, is_leaf: bool = False, idx = None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.is_leaf = is_leaf\n",
    "        self.value = sum(n.value for n in (left, right) if n is not None)\n",
    "        self.parent = None\n",
    "        self.idx = idx  # this value is only set for leaf nodes\n",
    "        if left is not None:\n",
    "            left.parent = self\n",
    "        if right is not None:\n",
    "            right.parent = self\n",
    "\n",
    "    @classmethod\n",
    "    def create_leaf(cls, value, idx):\n",
    "        leaf = cls(None, None, is_leaf=True, idx=idx)\n",
    "        leaf.value = value\n",
    "        return leaf\n",
    "\n",
    "\n",
    "def create_tree(input: list):\n",
    "    nodes = [Node.create_leaf(v, i) for i, v in enumerate(input)]\n",
    "    leaf_nodes = nodes\n",
    "    while len(nodes) > 1:\n",
    "        inodes = iter(nodes)\n",
    "        nodes = [Node(*pair) for pair in zip(inodes, inodes)]\n",
    "\n",
    "    return nodes[0], leaf_nodes\n",
    "\n",
    "def retrieve(value: float, node: Node):\n",
    "    if node.is_leaf:\n",
    "        return node\n",
    "\n",
    "    if node.left.value >= value:\n",
    "        return retrieve(value, node.left)\n",
    "    else:\n",
    "        return retrieve(value - node.left.value, node.right)\n",
    "\n",
    "def update(node: Node, new_value: float):\n",
    "    change = new_value - node.value\n",
    "\n",
    "    node.value = new_value\n",
    "    propagate_changes(change, node.parent)\n",
    "\n",
    "\n",
    "def propagate_changes(change: float, node: Node):\n",
    "    node.value += change\n",
    "\n",
    "    if node.parent is not None:\n",
    "        propagate_changes(change, node.parent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Controler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def huber_loss(loss):\n",
    "    return 0.5 * loss ** 2 if abs(loss) < 1.0 else abs(loss) - 0.5\n",
    "\n",
    "class Controler(object):\n",
    "    def __init__(self, min_dropout, max_dropout, dropout_min_iter, min_temp, max_temp, temp_min_iter, min_beta, max_beta, beta_min_iter,  gamma, batch_size, tau, num_actions, memory_samples, store_path, hidden_layers, delay_training, to_train):\n",
    "        self.to_train = to_train\n",
    "        self.delay_training = delay_training\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.store_path = store_path\n",
    "        self.memory_samples = memory_samples\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.beta_min_iter = beta_min_iter\n",
    "        self.max_beta = max_beta\n",
    "        self.min_beta = min_beta\n",
    "        self.temp_min_iter = temp_min_iter\n",
    "        self.max_temp = max_temp\n",
    "        self.min_temp = min_temp\n",
    "        self.dropout_min_iter = dropout_min_iter\n",
    "        self.max_dropout = max_dropout\n",
    "        self.min_dropout = min_dropout\n",
    "        self.num_actions = num_actions\n",
    "        self.primary_network = DQModel(self.hidden_layers, self.num_actions, True, self.max_dropout)\n",
    "        self.target_network = DQModel(self.hidden_layers, self.num_actions, True, 0.)\n",
    "        self.primary_network.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.Huber())\n",
    "        for t, e in zip(self.target_network.trainable_variables, self.primary_network.trainable_variables):\n",
    "            t.assign(e)\n",
    "        self.env = MillEnv()\n",
    "        self.agent = Agent()\n",
    "        self.memory = Memory(memory_samples)\n",
    "        self.steps = 0\n",
    "        self.temp = max_temp\n",
    "        self.dropoutRate = max_dropout\n",
    "        self.train_writer = tf.summary.create_file_writer(\"Tensorboard/\" + self.store_path)\n",
    "    def startGame(self):\n",
    "        self.env.reset()\n",
    "    def update_network(self):\n",
    "        # update target network parameters slowly from primary network\n",
    "        for t, e in zip(self.target_network.trainable_variables, self.primary_network.trainable_variables):\n",
    "            t.assign(t * (1 - self.tau) + e * self.tau)\n",
    "    def get_per_error(self, batch, primary_network, target_network):\n",
    "        states = np.array([val[0] for val in batch])\n",
    "        actions = np.array([val[1] for val in batch])\n",
    "        rewards = np.array([val[2] for val in batch])\n",
    "        next_states = np.array([val[3] for val in batch])\n",
    "        valid_idxs = np.array([val[4] for val in batch])\n",
    "        # predict Q(s,a) given the batch of states\n",
    "        prim_qt = primary_network(states)\n",
    "        # predict Q(s',a') from the evaluation network\n",
    "        prim_qtp1 = primary_network(next_states)\n",
    "        # copy the prim_qt tensor into the target_q tensor - we then will update one index corresponding to the max action\n",
    "        target_q = prim_qt.numpy()\n",
    "        updates = rewards\n",
    "        batch_idxs = np.arange(len(batch))\n",
    "        prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "        q_from_target = target_network(next_states)\n",
    "        updates[valid_idxs] += self.gamma * q_from_target.numpy()[batch_idxs[valid_idxs], prim_action_tp1[valid_idxs]]\n",
    "        target_q[batch_idxs, actions] = updates\n",
    "        error = [huber_loss(target_q[i, actions[i]] - prim_qt.numpy()[i, actions[i]])for i in range(states.shape[0])]\n",
    "        return target_q, error\n",
    "    def train_network(self, primary_network, memory, target_network):\n",
    "        batch, idxs, is_weights = memory.sample(self.batch_size)\n",
    "        states = np.array([val[0] for val in batch])\n",
    "        target_q, error = self.get_per_error(batch, primary_network, target_network )\n",
    "        for i in range(len(idxs)):\n",
    "            memory.update(idxs[i], error[i])\n",
    "        loss = primary_network.train_on_batch(states, target_q, is_weights)\n",
    "        return loss\n",
    "    def gameLoop(self, episode):\n",
    "        self.startGame()\n",
    "        totreward = 0.\n",
    "        last_state = [0,0]\n",
    "        last_action = [0,0]\n",
    "        reward = [0,0]\n",
    "        cnt = 1\n",
    "        avg_loss = 0\n",
    "        invalid_moves = 0\n",
    "        while True:\n",
    "            if self.env.moveNeeded == self.to_train:\n",
    "                self.primary_network.changeDropoutRate(self.dropoutRate)\n",
    "                actual_player = self.env.isPlaying\n",
    "                last_state[1 if actual_player == 1 else 0] = self.env.getBoard(), self.env.getSummary(actual_player)\n",
    "                pos= self.agent.getPos(self.agent.processState(actual_player, self.env.getBoard(), self.to_train, self.env.selected), self.temp, self.env.moveNeeded,self.primary_network)\n",
    "                valid = self.env.makeMove(pos)\n",
    "                reward[1 if actual_player == 1 else 0] = self.getReward(last_state[1 if actual_player == 1 else 0][1], self.env.getSummary(actual_player), valid)\n",
    "                last_action[1 if actual_player == 1 else 0] = pos, valid\n",
    "                finished = self.env.isFinished()\n",
    "                if last_state[1 if actual_player == -1 else 0] != 0 and valid:\n",
    "                    new_state = self.env.getBoard().flatten(), self.env.getSummary(-actual_player)\n",
    "                    rewardAbs = self.getReward(last_state[1 if actual_player == -1 else 0][1], new_state[1], last_action[1 if actual_player == -1 else 0][1]) if finished != 0 else reward[1 if actual_player == -1 else 0]\n",
    "                    totreward += rewardAbs\n",
    "                    sample = (self.agent.processState(-actual_player,last_state[1 if actual_player == -1 else 0][0], self.to_train, self.env.selected), last_action[1 if actual_player == -1 else 0][0], rewardAbs, self.agent.processState(-actual_player,new_state[0], self.to_train, self.env.selected), finished == 0)\n",
    "                    _, error = self.get_per_error([sample], self.primary_network, self.target_network)\n",
    "                    self.memory.append(sample,error[0])\n",
    "                if self.steps > self.delay_training:\n",
    "                    loss = self.train_network(self.primary_network, self.memory, self.target_network)\n",
    "                    self.update_network()\n",
    "                else:\n",
    "                    loss = 0\n",
    "                avg_loss += loss\n",
    "                if self.steps > self.delay_training:\n",
    "                    self.temp = self.max_temp - ((self.steps - self.delay_training) / self.temp_min_iter) * \\\n",
    "                      (self.max_temp - self.min_temp) if self.steps < self.temp_min_iter else \\\n",
    "                    self.min_temp\n",
    "                    self.dropoutRate = self.max_dropout - ((self.steps - self.delay_training) / self.dropout_min_iter) * \\\n",
    "                      (self.max_dropout - self.min_dropout) if self.steps < self.dropout_min_iter else \\\n",
    "                    self.min_dropout\n",
    "                    beta = self.min_beta + ((self.steps - self.delay_training) / self.beta_min_iter) * \\\n",
    "                      (self.max_beta - self.min_beta) if self.steps < self.beta_min_iter else \\\n",
    "                    self.max_beta\n",
    "                    self.memory.beta = beta\n",
    "                self.steps += 1\n",
    "                if cnt >= 1000:\n",
    "                    finished = 2\n",
    "                if  not valid or finished != 0:\n",
    "                    invalid_moves += 0 if finished != 0 else 1\n",
    "                    new_state = self.env.getBoard()\n",
    "                    totreward += reward[1 if actual_player == 1 else 0]\n",
    "                    sample = (self.agent.processState(actual_player, last_state[1 if actual_player == 1 else 0][0], self.to_train, self.env.selected), last_action[1 if actual_player == 1 else 0][0], reward[1 if actual_player == 1 else 0],self.agent.processState(actual_player,new_state, self.to_train, self.env.selected), False)\n",
    "                    _, error = self.get_per_error([sample], self.primary_network, self.target_network)\n",
    "                    self.memory.append(sample, error[0])\n",
    "                cnt += 1\n",
    "            else:\n",
    "                self.env.makeMove(self.agent.getPos(self.env.getBoard(), 1,self.env.moveNeeded))\n",
    "            if self.env.isFinished() != 0:\n",
    "                finished = self.env.isFinished()\n",
    "                if self.steps > self.delay_training:\n",
    "                    avg_loss /= cnt\n",
    "                    print(f\"Episode: {episode}, Reward: {totreward:.1f}, avg loss: {avg_loss:.3f}, eps: {self.temp:.3f}, dropout: {self.dropoutRate:.3f}, beta: {self.memory.beta:.3f}\")\n",
    "                else:\n",
    "                    avg_loss = 0\n",
    "                    print(f\"Pre-Training... Episode {episode}\")\n",
    "                if episode == 0:\n",
    "                    tf.summary.trace_on()\n",
    "                    self.primary_network.traceable(self.agent.processState(1, last_state[1][0],self.to_train, 0).reshape(1,-1))\n",
    "                with self.train_writer.as_default():\n",
    "                    tf.summary.scalar('reward', totreward, step=episode)\n",
    "                    tf.summary.scalar('avg loss', avg_loss, step=episode)\n",
    "                    tf.summary.scalar('invalid moves', invalid_moves, step=episode)\n",
    "                    if episode == 0:\n",
    "                        tf.summary.trace_export('model', step=0)\n",
    "                break\n",
    "        return finished, reward\n",
    "    def train(self, episodes: int):\n",
    "        for index in range(episodes):\n",
    "            self.gameLoop(index)\n",
    "        self.primary_network.save_weights(f\"models/\" + self.store_path, save_format=\"tf\")\n",
    "    def getReward(self, last_state, new_state, valid) -> float:\n",
    "        reward = 0.\n",
    "        if last_state[0] < new_state[0]:\n",
    "            return 4.\n",
    "        elif last_state[1] < new_state[1]:\n",
    "            return -1.\n",
    "        elif not valid:\n",
    "            return -5.\n",
    "        if last_state[3] == new_state[3] and last_state[3] > 0:\n",
    "            reward += -1.\n",
    "        if last_state[2] < new_state[2]:\n",
    "            reward += 2.\n",
    "        if last_state[3] > new_state[3]:\n",
    "            reward += 2.\n",
    "        return reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 13 14]\n",
      " [ 2 14 23]]\n",
      "[ 1.  1.  1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n",
      "3\n",
      "False\n",
      "[ 1.  0.  0. -1.]\n",
      "(1, 0, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "env = MillEnv()\n",
    "print(env.getInRows(14))\n",
    "\n",
    "env.makeMove(0)\n",
    "env.makeMove(3)\n",
    "env.makeMove(2)\n",
    "env.makeMove(4)\n",
    "env.makeMove(1)\n",
    "print(env.getBoard())\n",
    "print(env.moveNeeded)\n",
    "ThreeInChosenRow = abs(env.board[env.getInRows(0)].sum(axis=1)) == 3\n",
    "print(~ThreeInChosenRow.any())\n",
    "print(env.getMoveFields(4))\n",
    "print(env.getSummary(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n",
      "Gewonnen hat -1\n",
      "Gewonnen hat 1\n",
      "Gewonnen hat -1\n",
      "Gewonnen hat 1\n",
      "Gewonnen hat 1\n",
      "Gewonnen hat 1\n",
      "Gewonnen hat 1\n",
      "Gewonnen hat -1\n",
      "Gewonnen hat -1\n",
      "Gewonnen hat -1\n",
      "3.0672783851623535\n",
      "0.3067352056503296\n"
     ]
    }
   ],
   "source": [
    "env = MillEnv()\n",
    "ag = Agent()\n",
    "print(ag.processState(1,env.getBoard(), 2,3).shape)\n",
    "firstTime = time.time()\n",
    "iters = 10\n",
    "for i in range(iters):\n",
    "    env.reset()\n",
    "    while env.isFinished() == 0:\n",
    "        valid = env.makeMove(ag.getPos(env.getBoard(), 0, env.moveNeeded))\n",
    "    print(f\"Gewonnen hat {env.isFinished()}\")\n",
    "print(time.time()- firstTime)\n",
    "print((time.time()-firstTime)/iters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Displayer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "displayer = MillDisplayer()\n",
    "displayer.windowsLoop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Moderated  Play"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "moderateGraphics = ModeratedGraphics()\n",
    "moderateGraphics.playLoop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Controller"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dq_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dq_model_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Pre-Training... Episode 0\n",
      "Episode: 1, Reward: -192.0, avg loss: 0.180, eps: 2.998, dropout: 0.400, beta: 0.400\n",
      "Episode: 2, Reward: -81.0, avg loss: 0.057, eps: 2.995, dropout: 0.399, beta: 0.401\n",
      "Episode: 3, Reward: -132.0, avg loss: 0.052, eps: 2.993, dropout: 0.399, beta: 0.402\n",
      "Episode: 4, Reward: -112.0, avg loss: 0.050, eps: 2.991, dropout: 0.398, beta: 0.402\n",
      "Episode: 5, Reward: -66.0, avg loss: 0.053, eps: 2.989, dropout: 0.398, beta: 0.403\n",
      "Episode: 6, Reward: -44.0, avg loss: 0.054, eps: 2.988, dropout: 0.398, beta: 0.403\n",
      "Episode: 7, Reward: -86.0, avg loss: 0.052, eps: 2.986, dropout: 0.397, beta: 0.404\n",
      "Episode: 8, Reward: -83.0, avg loss: 0.049, eps: 2.985, dropout: 0.397, beta: 0.404\n",
      "Episode: 9, Reward: -37.0, avg loss: 0.057, eps: 2.983, dropout: 0.397, beta: 0.404\n",
      "Episode: 10, Reward: -81.0, avg loss: 0.047, eps: 2.982, dropout: 0.397, beta: 0.405\n",
      "Episode: 11, Reward: 40.0, avg loss: 0.054, eps: 2.975, dropout: 0.395, beta: 0.407\n",
      "Episode: 12, Reward: -51.0, avg loss: 0.052, eps: 2.974, dropout: 0.395, beta: 0.407\n",
      "Episode: 13, Reward: -120.0, avg loss: 0.050, eps: 2.972, dropout: 0.395, beta: 0.407\n",
      "Episode: 14, Reward: -81.0, avg loss: 0.052, eps: 2.970, dropout: 0.395, beta: 0.408\n",
      "Episode: 15, Reward: -118.0, avg loss: 0.050, eps: 2.962, dropout: 0.393, beta: 0.410\n",
      "Episode: 16, Reward: -224.0, avg loss: 0.046, eps: 2.958, dropout: 0.392, beta: 0.411\n",
      "Episode: 17, Reward: -38.0, avg loss: 0.043, eps: 2.957, dropout: 0.392, beta: 0.411\n",
      "Episode: 18, Reward: -113.0, avg loss: 0.043, eps: 2.955, dropout: 0.392, beta: 0.412\n",
      "Episode: 19, Reward: -19.0, avg loss: 0.043, eps: 2.954, dropout: 0.392, beta: 0.412\n",
      "Episode: 20, Reward: -49.0, avg loss: 0.043, eps: 2.952, dropout: 0.391, beta: 0.412\n",
      "Episode: 21, Reward: -60.0, avg loss: 0.042, eps: 2.951, dropout: 0.391, beta: 0.413\n",
      "Episode: 22, Reward: -71.0, avg loss: 0.041, eps: 2.949, dropout: 0.391, beta: 0.413\n",
      "Episode: 23, Reward: -121.0, avg loss: 0.045, eps: 2.947, dropout: 0.390, beta: 0.414\n",
      "Episode: 24, Reward: -50.0, avg loss: 0.049, eps: 2.946, dropout: 0.390, beta: 0.414\n",
      "Episode: 25, Reward: -75.0, avg loss: 0.049, eps: 2.944, dropout: 0.390, beta: 0.415\n",
      "Episode: 26, Reward: -84.0, avg loss: 0.046, eps: 2.942, dropout: 0.389, beta: 0.415\n",
      "Episode: 27, Reward: -60.0, avg loss: 0.045, eps: 2.941, dropout: 0.389, beta: 0.415\n",
      "Episode: 28, Reward: -55.0, avg loss: 0.043, eps: 2.940, dropout: 0.389, beta: 0.416\n",
      "Episode: 29, Reward: -71.0, avg loss: 0.044, eps: 2.938, dropout: 0.389, beta: 0.416\n",
      "Episode: 30, Reward: -46.0, avg loss: 0.043, eps: 2.937, dropout: 0.388, beta: 0.416\n",
      "Episode: 31, Reward: 166.0, avg loss: 0.053, eps: 2.929, dropout: 0.387, beta: 0.419\n",
      "Episode: 32, Reward: -9.0, avg loss: 0.048, eps: 2.925, dropout: 0.386, beta: 0.419\n",
      "Episode: 33, Reward: -98.0, avg loss: 0.042, eps: 2.924, dropout: 0.386, beta: 0.420\n",
      "Episode: 34, Reward: -43.0, avg loss: 0.045, eps: 2.922, dropout: 0.386, beta: 0.420\n",
      "Episode: 35, Reward: -28.0, avg loss: 0.043, eps: 2.919, dropout: 0.385, beta: 0.421\n",
      "Episode: 36, Reward: -222.0, avg loss: 0.044, eps: 2.903, dropout: 0.382, beta: 0.425\n",
      "Episode: 37, Reward: -133.0, avg loss: 0.036, eps: 2.900, dropout: 0.382, beta: 0.426\n",
      "Episode: 38, Reward: -108.0, avg loss: 0.041, eps: 2.895, dropout: 0.381, beta: 0.427\n",
      "Episode: 39, Reward: -26.0, avg loss: 0.038, eps: 2.893, dropout: 0.380, beta: 0.428\n",
      "Episode: 40, Reward: 30.0, avg loss: 0.039, eps: 2.873, dropout: 0.377, beta: 0.433\n",
      "Episode: 41, Reward: -293.0, avg loss: 0.042, eps: 2.868, dropout: 0.376, beta: 0.434\n",
      "Episode: 42, Reward: -9.0, avg loss: 0.033, eps: 2.867, dropout: 0.376, beta: 0.435\n",
      "Episode: 43, Reward: -76.0, avg loss: 0.039, eps: 2.865, dropout: 0.375, beta: 0.435\n",
      "Episode: 44, Reward: -137.0, avg loss: 0.035, eps: 2.863, dropout: 0.375, beta: 0.436\n",
      "Episode: 45, Reward: -84.0, avg loss: 0.050, eps: 2.861, dropout: 0.375, beta: 0.436\n",
      "Episode: 46, Reward: -162.0, avg loss: 0.041, eps: 2.859, dropout: 0.374, beta: 0.437\n",
      "Episode: 47, Reward: -28.0, avg loss: 0.033, eps: 2.857, dropout: 0.374, beta: 0.437\n",
      "Episode: 48, Reward: -60.0, avg loss: 0.038, eps: 2.856, dropout: 0.374, beta: 0.438\n",
      "Episode: 49, Reward: -61.0, avg loss: 0.087, eps: 2.855, dropout: 0.373, beta: 0.438\n",
      "Episode: 50, Reward: -118.0, avg loss: 0.064, eps: 2.852, dropout: 0.373, beta: 0.439\n",
      "Episode: 51, Reward: -58.0, avg loss: 0.051, eps: 2.851, dropout: 0.373, beta: 0.439\n",
      "Episode: 52, Reward: -51.0, avg loss: 0.043, eps: 2.850, dropout: 0.373, beta: 0.439\n",
      "Episode: 53, Reward: -91.0, avg loss: 0.312, eps: 2.848, dropout: 0.372, beta: 0.440\n",
      "Episode: 54, Reward: -34.0, avg loss: 0.048, eps: 2.846, dropout: 0.372, beta: 0.440\n",
      "Episode: 55, Reward: -56.0, avg loss: 0.047, eps: 2.844, dropout: 0.372, beta: 0.441\n",
      "Episode: 56, Reward: -108.0, avg loss: 0.046, eps: 2.839, dropout: 0.371, beta: 0.442\n",
      "Episode: 57, Reward: -70.0, avg loss: 0.044, eps: 2.838, dropout: 0.370, beta: 0.442\n",
      "Episode: 58, Reward: -113.0, avg loss: 0.043, eps: 2.835, dropout: 0.370, beta: 0.443\n",
      "Episode: 59, Reward: -103.0, avg loss: 0.040, eps: 2.833, dropout: 0.370, beta: 0.443\n",
      "Episode: 60, Reward: -107.0, avg loss: 0.044, eps: 2.832, dropout: 0.369, beta: 0.444\n",
      "Episode: 61, Reward: -52.0, avg loss: 0.053, eps: 2.830, dropout: 0.369, beta: 0.444\n",
      "Episode: 62, Reward: -414.0, avg loss: 0.042, eps: 2.825, dropout: 0.368, beta: 0.446\n",
      "Episode: 63, Reward: -65.0, avg loss: 0.039, eps: 2.824, dropout: 0.368, beta: 0.446\n",
      "Episode: 64, Reward: 93.0, avg loss: 0.044, eps: 2.818, dropout: 0.367, beta: 0.447\n",
      "Episode: 65, Reward: 28.0, avg loss: 0.060, eps: 2.814, dropout: 0.366, beta: 0.448\n",
      "Episode: 66, Reward: -51.0, avg loss: 0.046, eps: 2.813, dropout: 0.366, beta: 0.449\n",
      "Episode: 67, Reward: -93.0, avg loss: 0.048, eps: 2.811, dropout: 0.366, beta: 0.449\n",
      "Episode: 68, Reward: -70.0, avg loss: 0.041, eps: 2.810, dropout: 0.365, beta: 0.450\n",
      "Episode: 69, Reward: -68.0, avg loss: 0.045, eps: 2.808, dropout: 0.365, beta: 0.450\n",
      "Episode: 70, Reward: -68.0, avg loss: 0.045, eps: 2.805, dropout: 0.364, beta: 0.451\n",
      "Episode: 71, Reward: -38.0, avg loss: 0.039, eps: 2.804, dropout: 0.364, beta: 0.451\n",
      "Episode: 72, Reward: -59.0, avg loss: 0.044, eps: 2.803, dropout: 0.364, beta: 0.451\n",
      "Episode: 73, Reward: -315.0, avg loss: 0.047, eps: 2.798, dropout: 0.363, beta: 0.453\n",
      "Episode: 74, Reward: -288.0, avg loss: 0.040, eps: 2.786, dropout: 0.361, beta: 0.456\n",
      "Episode: 75, Reward: -49.0, avg loss: 0.065, eps: 2.784, dropout: 0.361, beta: 0.456\n",
      "Episode: 76, Reward: -108.0, avg loss: 0.048, eps: 2.782, dropout: 0.360, beta: 0.457\n",
      "Episode: 77, Reward: -51.0, avg loss: 0.039, eps: 2.781, dropout: 0.360, beta: 0.457\n",
      "Episode: 78, Reward: -41.0, avg loss: 0.039, eps: 2.779, dropout: 0.360, beta: 0.458\n",
      "Episode: 79, Reward: -56.0, avg loss: 0.043, eps: 2.778, dropout: 0.359, beta: 0.458\n",
      "Episode: 80, Reward: -79.0, avg loss: 0.037, eps: 2.775, dropout: 0.359, beta: 0.459\n",
      "Episode: 81, Reward: -169.0, avg loss: 0.037, eps: 2.772, dropout: 0.358, beta: 0.459\n",
      "Episode: 82, Reward: -48.0, avg loss: 0.035, eps: 2.771, dropout: 0.358, beta: 0.460\n",
      "Episode: 83, Reward: -90.0, avg loss: 0.033, eps: 2.769, dropout: 0.358, beta: 0.460\n",
      "Episode: 84, Reward: -177.0, avg loss: 0.035, eps: 2.766, dropout: 0.357, beta: 0.461\n",
      "Episode: 85, Reward: -132.0, avg loss: 0.033, eps: 2.764, dropout: 0.357, beta: 0.462\n",
      "Episode: 86, Reward: -44.0, avg loss: 0.036, eps: 2.763, dropout: 0.357, beta: 0.462\n",
      "Episode: 87, Reward: -50.0, avg loss: 0.036, eps: 2.762, dropout: 0.356, beta: 0.462\n",
      "Episode: 88, Reward: -45.0, avg loss: 0.033, eps: 2.761, dropout: 0.356, beta: 0.462\n",
      "Episode: 89, Reward: -304.0, avg loss: 0.036, eps: 2.756, dropout: 0.355, beta: 0.464\n",
      "Episode: 90, Reward: 6.0, avg loss: 0.038, eps: 2.753, dropout: 0.355, beta: 0.465\n",
      "Episode: 91, Reward: -36.0, avg loss: 0.035, eps: 2.751, dropout: 0.355, beta: 0.465\n",
      "Episode: 92, Reward: -27.0, avg loss: 0.038, eps: 2.750, dropout: 0.354, beta: 0.465\n",
      "Episode: 93, Reward: -167.0, avg loss: 0.039, eps: 2.747, dropout: 0.354, beta: 0.466\n",
      "Episode: 94, Reward: -130.0, avg loss: 0.040, eps: 2.745, dropout: 0.353, beta: 0.466\n",
      "Episode: 95, Reward: -69.0, avg loss: 0.037, eps: 2.744, dropout: 0.353, beta: 0.467\n",
      "Episode: 96, Reward: -169.0, avg loss: 0.040, eps: 2.742, dropout: 0.353, beta: 0.467\n",
      "Episode: 97, Reward: -93.0, avg loss: 0.036, eps: 2.740, dropout: 0.353, beta: 0.468\n",
      "Episode: 98, Reward: -41.0, avg loss: 0.034, eps: 2.739, dropout: 0.352, beta: 0.468\n",
      "Episode: 99, Reward: -101.0, avg loss: 0.035, eps: 2.737, dropout: 0.352, beta: 0.469\n",
      "Episode: 100, Reward: -58.0, avg loss: 0.040, eps: 2.736, dropout: 0.352, beta: 0.469\n",
      "Episode: 101, Reward: -120.0, avg loss: 0.185, eps: 2.733, dropout: 0.351, beta: 0.470\n",
      "Episode: 102, Reward: -353.0, avg loss: 0.098, eps: 2.726, dropout: 0.350, beta: 0.471\n",
      "Episode: 103, Reward: -29.0, avg loss: 0.041, eps: 2.725, dropout: 0.350, beta: 0.472\n",
      "Episode: 104, Reward: -135.0, avg loss: 0.040, eps: 2.723, dropout: 0.349, beta: 0.472\n",
      "Episode: 105, Reward: -19.0, avg loss: 0.036, eps: 2.721, dropout: 0.349, beta: 0.473\n",
      "Episode: 106, Reward: -69.0, avg loss: 0.046, eps: 2.720, dropout: 0.349, beta: 0.473\n",
      "Episode: 107, Reward: -67.0, avg loss: 0.046, eps: 2.718, dropout: 0.349, beta: 0.473\n",
      "Episode: 108, Reward: -91.0, avg loss: 0.040, eps: 2.717, dropout: 0.348, beta: 0.474\n",
      "Episode: 109, Reward: 14.0, avg loss: 0.031, eps: 2.715, dropout: 0.348, beta: 0.474\n",
      "Episode: 110, Reward: -364.0, avg loss: 0.044, eps: 2.702, dropout: 0.346, beta: 0.478\n",
      "Episode: 111, Reward: -147.0, avg loss: 0.036, eps: 2.699, dropout: 0.345, beta: 0.479\n",
      "Episode: 112, Reward: -48.0, avg loss: 0.033, eps: 2.698, dropout: 0.345, beta: 0.479\n",
      "Episode: 113, Reward: -132.0, avg loss: 0.036, eps: 2.696, dropout: 0.344, beta: 0.479\n",
      "Episode: 114, Reward: -34.0, avg loss: 0.042, eps: 2.695, dropout: 0.344, beta: 0.480\n",
      "Episode: 115, Reward: -196.0, avg loss: 0.048, eps: 2.691, dropout: 0.344, beta: 0.481\n",
      "Episode: 116, Reward: -20.0, avg loss: 0.181, eps: 2.690, dropout: 0.343, beta: 0.481\n",
      "Episode: 117, Reward: -41.0, avg loss: 0.184, eps: 2.689, dropout: 0.343, beta: 0.481\n",
      "Episode: 118, Reward: -24.0, avg loss: 0.690, eps: 2.687, dropout: 0.343, beta: 0.482\n",
      "Episode: 119, Reward: 92.0, avg loss: 0.055, eps: 2.680, dropout: 0.342, beta: 0.483\n",
      "Episode: 120, Reward: -71.0, avg loss: 0.037, eps: 2.679, dropout: 0.341, beta: 0.484\n",
      "Episode: 121, Reward: -98.0, avg loss: 0.042, eps: 2.677, dropout: 0.341, beta: 0.484\n",
      "Episode: 122, Reward: -109.0, avg loss: 0.043, eps: 2.675, dropout: 0.341, beta: 0.485\n",
      "Episode: 123, Reward: 5.0, avg loss: 0.051, eps: 2.674, dropout: 0.340, beta: 0.485\n",
      "Episode: 124, Reward: -118.0, avg loss: 0.040, eps: 2.672, dropout: 0.340, beta: 0.486\n",
      "Episode: 125, Reward: -120.0, avg loss: 0.047, eps: 2.670, dropout: 0.340, beta: 0.486\n",
      "Episode: 126, Reward: -148.0, avg loss: 0.047, eps: 2.667, dropout: 0.339, beta: 0.487\n",
      "Episode: 127, Reward: -34.0, avg loss: 0.037, eps: 2.666, dropout: 0.339, beta: 0.487\n",
      "Episode: 128, Reward: -106.0, avg loss: 0.042, eps: 2.664, dropout: 0.339, beta: 0.488\n",
      "Episode: 129, Reward: -46.0, avg loss: 0.045, eps: 2.663, dropout: 0.338, beta: 0.488\n",
      "Episode: 130, Reward: -134.0, avg loss: 0.044, eps: 2.660, dropout: 0.338, beta: 0.489\n",
      "Episode: 131, Reward: -5.0, avg loss: 0.046, eps: 2.659, dropout: 0.338, beta: 0.489\n",
      "Episode: 132, Reward: -142.0, avg loss: 0.040, eps: 2.657, dropout: 0.337, beta: 0.490\n",
      "Episode: 133, Reward: -58.0, avg loss: 0.036, eps: 2.655, dropout: 0.337, beta: 0.490\n",
      "Episode: 134, Reward: -108.0, avg loss: 0.041, eps: 2.653, dropout: 0.337, beta: 0.491\n",
      "Episode: 135, Reward: -125.0, avg loss: 0.039, eps: 2.650, dropout: 0.336, beta: 0.491\n",
      "Episode: 136, Reward: -81.0, avg loss: 0.037, eps: 2.648, dropout: 0.336, beta: 0.492\n",
      "Episode: 137, Reward: -140.0, avg loss: 0.033, eps: 2.646, dropout: 0.335, beta: 0.492\n",
      "Episode: 138, Reward: -36.0, avg loss: 0.038, eps: 2.645, dropout: 0.335, beta: 0.493\n",
      "Episode: 139, Reward: -147.0, avg loss: 0.037, eps: 2.643, dropout: 0.335, beta: 0.493\n",
      "Episode: 140, Reward: -185.0, avg loss: 0.041, eps: 2.640, dropout: 0.334, beta: 0.494\n",
      "Episode: 141, Reward: 6.0, avg loss: 0.040, eps: 2.639, dropout: 0.334, beta: 0.494\n"
     ]
    }
   ],
   "source": [
    "cont = Controler(0.05,0.4,50000,0.7,3,60000,0.4,1,60000, 0.5, 32,0.1,24,100000,f\"Move0-{dt.datetime.now().strftime('%d%m%Y%H%M')}\", 256,100,0)\n",
    "cont.train(2000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}